{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata sources:\\n1. google mobility: https://www.google.com/covid19/mobility/\\n2. apple mobilty: https://covid19.apple.com/mobility\\n6a. positiveIncrease: https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series\\n6b. negaiveResults, totalRes: https://healthdata.gov/dataset/COVID-19-Diagnostic-Laboratory-Testing-PCR-Testing/j8mb-icvb\\n7. jhu deaths: https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series\\n9. excess deaths: https://www.cdc.gov/nchs/nvss/vsrr/covid19/excess_deaths.htm\\n13. vaccine doses: https://github.com/govex/COVID-19/blob/master/data_tables/vaccine_data/us_data/time_series/vaccine_data_us_timeline.csv\\n5. fb-google survey: deplhi api\\n\\nDATA PROVIDED BY ALEX\\n11. covidnet: original cdc, processed data to use given by ALEX [https://gis.cdc.gov/grasp/covidnet/COVID19_3.html] \\n12. CDC hospitalized: https://healthdata.gov/dataset/covid-19-reported-patient-impact-and-hospital-capacity-state-timeseries\\n\\nDATA OBSOLETE-- NOT COLLECTING ANYMORE\\n3. Covid ExposureIndex dex: https://github.com/COVIDExposureIndices/COVIDExposureIndices\\n4. kinsa: kinsa_pull.ipynb \\n6. hospitalization: https://covidtracking.com/api\\n8. iqvia: Alex from Jimeng\\n10. Emergency visits (less priority: region level): \\n#https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/09042020/covid-like-illness.html\\nhttps://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/10092020/outpatient-emergency-visits.html\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data sources:\n",
    "1. google mobility: https://www.google.com/covid19/mobility/\n",
    "2. apple mobilty: https://covid19.apple.com/mobility\n",
    "6a. positiveIncrease: https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series\n",
    "6b. negaiveResults, totalRes: https://healthdata.gov/dataset/COVID-19-Diagnostic-Laboratory-Testing-PCR-Testing/j8mb-icvb\n",
    "7. jhu deaths: https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series\n",
    "9. excess deaths: https://www.cdc.gov/nchs/nvss/vsrr/covid19/excess_deaths.htm\n",
    "13. vaccine doses: https://github.com/govex/COVID-19/blob/master/data_tables/vaccine_data/us_data/time_series/vaccine_data_us_timeline.csv\n",
    "5. fb-google survey: deplhi api\n",
    "\n",
    "DATA PROVIDED BY ALEX\n",
    "11. covidnet: original cdc, processed data to use given by ALEX [https://gis.cdc.gov/grasp/covidnet/COVID19_3.html] \n",
    "12. CDC hospitalized: https://healthdata.gov/dataset/covid-19-reported-patient-impact-and-hospital-capacity-state-timeseries\n",
    "\n",
    "DATA OBSOLETE-- NOT COLLECTING ANYMORE\n",
    "3. Covid ExposureIndex dex: https://github.com/COVIDExposureIndices/COVIDExposureIndices\n",
    "4. kinsa: kinsa_pull.ipynb \n",
    "6. hospitalization: https://covidtracking.com/api\n",
    "8. iqvia: Alex from Jimeng\n",
    "10. Emergency visits (less priority: region level): \n",
    "#https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/09042020/covid-like-illness.html\n",
    "https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/10092020/outpatient-emergency-visits.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "\n",
    "from epiweeks import Week, Year\n",
    "from delphi_epidata import Epidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_test(data_dic,cols,epiweek_date,state_index,outfile):\n",
    "    state_names=list(state_index.keys())\n",
    "    all_cols=['date','region']+cols\n",
    "    out_data=pd.DataFrame(columns=all_cols)\n",
    "    for st in range(len(state_names)):\n",
    "        temp_data=pd.DataFrame(columns=all_cols)\n",
    "        temp_data['date']=epiweek_date\n",
    "        temp_data['region']=[state_names[st]]*len(epiweek_date)\n",
    "        for c in cols:\n",
    "            temp_data[c]=data_dic[c][st][:]\n",
    "        out_data=out_data.append(temp_data,ignore_index=True)\n",
    "    out_data=out_data[all_cols]\n",
    "    print(out_data.shape)\n",
    "    out_data.to_csv(outfile,index=False)\n",
    "    print('output file written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_survey_epidata(col_name,epidata,state_index,state_names,epiweek_date):\n",
    "    week_cases=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    total_sample=0\n",
    "    for ix in range(len(epidata)):\n",
    "        row=epidata[ix]\n",
    "        name=row['geo_value'].upper()\n",
    "        w_idx=find_week_index(epiweek_date,str(row['time_value']))\n",
    "        if name in state_names and w_idx!=-1:\n",
    "            state_id=state_index[name]\n",
    "            week_cases[state_id][w_idx]+=row['value']\n",
    "            week_cases[0][w_idx]+=(row['value']/100)*row['sample_size']\n",
    "            total_sample+=row['sample_size']\n",
    "    \n",
    "    #national\n",
    "    week_cases[0]=week_cases[0]*100/total_sample\n",
    "    \n",
    "    return week_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_delphi_vaccine(state_index,epiweek_date,start_week,end_week):\n",
    "    #'''\n",
    "    vacc11=Epidata.covidcast('fb-survey','smoothed_wcovid_vaccinated','day','state',[start_week, Epidata.range(start_week, 20210301)],'*')\n",
    "    vacc21=Epidata.covidcast('fb-survey','smoothed_wtested_positive_14d','day','state',[start_week, Epidata.range(start_week, 20210301)],'*')\n",
    "    vacc31=Epidata.covidcast('fb-survey','smoothed_wwearing_mask','day','state',[start_week, Epidata.range(start_week, end_week)],'*')\n",
    "    vacc41=Epidata.covidcast('fb-survey','smoothed_wtravel_outside_state_5d','day','state',[start_week, Epidata.range(start_week, 20210301)],'*')\n",
    "    vacc51=Epidata.covidcast('fb-survey','smoothed_wspent_time_1d','day','state',[start_week, Epidata.range(start_week, 20210301)],'*')\n",
    "    \n",
    "    vacc12=Epidata.covidcast('fb-survey','smoothed_wcovid_vaccinated','day','state',[20210301, Epidata.range(20210301, 20210501)],'*')\n",
    "    vacc13=Epidata.covidcast('fb-survey','smoothed_wcovid_vaccinated','day','state',[20210501, Epidata.range(20210501, 20210701)],'*')\n",
    "    vacc14=Epidata.covidcast('fb-survey','smoothed_wcovid_vaccinated','day','state',[20210701, Epidata.range(20210701, 20210901)],'*')\n",
    "    vacc15=Epidata.covidcast('fb-survey','smoothed_wcovid_vaccinated','day','state',[20210901, Epidata.range(20210901, 20211101)],'*')\n",
    "    vacc16=Epidata.covidcast('fb-survey','smoothed_wcovid_vaccinated','day','state',[20211101, Epidata.range(20211101, 20220101)],'*')\n",
    "    vacc17=Epidata.covidcast('fb-survey','smoothed_wcovid_vaccinated','day','state',[20220101, Epidata.range(20220101, end_week)],'*')\n",
    "    \n",
    "    vacc22=Epidata.covidcast('fb-survey','smoothed_wtested_positive_14d','day','state',[20210301, Epidata.range(20210301, 20210601)],'*')\n",
    "    vacc23=Epidata.covidcast('fb-survey','smoothed_wtested_positive_14d','day','state',[20210601, Epidata.range(20210601, 20210901)],'*')\n",
    "    vacc24=Epidata.covidcast('fb-survey','smoothed_wtested_positive_14d','day','state',[20210601, Epidata.range(20210901, 20211101)],'*')\n",
    "    vacc25=Epidata.covidcast('fb-survey','smoothed_wtested_positive_14d','day','state',[20210601, Epidata.range(20211101, 20220101)],'*')\n",
    "    vacc26=Epidata.covidcast('fb-survey','smoothed_wtested_positive_14d','day','state',[20220101, Epidata.range(20220101, end_week)],'*')\n",
    "\n",
    "    #vacc32=Epidata.covidcast('fb-survey','smoothed_wearing_mask','day','state',[20210301, Epidata.range(20210301, end_week)],'*')\n",
    "    vacc42=Epidata.covidcast('fb-survey','smoothed_wtravel_outside_state_5d','day','state',[20210301, Epidata.range(20210301, end_week)],'*')\n",
    "    vacc52=Epidata.covidcast('fb-survey','smoothed_wspent_time_1d','day','state',[20210301, Epidata.range(20210301, end_week)],'*')\n",
    "    \n",
    "    vacc1=vacc11['epidata']+vacc12['epidata']+vacc13['epidata']+vacc14['epidata']+vacc15['epidata']+vacc16['epidata']+vacc17['epidata']\n",
    "    vacc2=vacc21['epidata']+vacc22['epidata']+vacc23['epidata']+vacc24['epidata']+vacc25['epidata']+vacc26['epidata']\n",
    "    vacc3=vacc31['epidata']#+vacc32['epidata']\n",
    "    vacc4=vacc41['epidata']+vacc42['epidata']\n",
    "    vacc5=vacc51['epidata']+vacc52['epidata']\n",
    "    \n",
    "    print('smoothed_wcovid_vaccinated',vacc17['result'], vacc17['message'], len(vacc17['epidata']))\n",
    "    print('smoothed_wtested_positive_14d',vacc26['result'], vacc26['message'], len(vacc26['epidata']))\n",
    "    print('smoothed_wwearing_mask',vacc31['result'], vacc31['message'], len(vacc31['epidata']))\n",
    "    print('smoothed_wtravel_outside_state_5d',vacc42['result'], vacc42['message'], len(vacc42['epidata']))\n",
    "    print('smoothed_wspent_time_1d',vacc52['result'], vacc52['message'], len(vacc52['epidata']))\n",
    "    #'''\n",
    "    \n",
    "    state_names=list(state_index.keys())\n",
    "    cols=['smoothed_wcovid_vaccinated','smoothed_wtested_positive_14d','smoothed_wwearing_mask',\n",
    "          'smoothed_wtravel_outside_state_5d','smoothed_wspent_time_1d']\n",
    "    week_cases={}\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    #'''\n",
    "    week_cases[cols[0]]=read_survey_epidata(cols[0],vacc1,state_index,state_names,epiweek_date)\n",
    "    week_cases[cols[1]]=read_survey_epidata(cols[1],vacc2,state_index,state_names,epiweek_date)\n",
    "    week_cases[cols[2]]=read_survey_epidata(cols[2],vacc3,state_index,state_names,epiweek_date)\n",
    "    week_cases[cols[3]]=read_survey_epidata(cols[3],vacc4,state_index,state_names,epiweek_date)\n",
    "    week_cases[cols[4]]=read_survey_epidata(cols[4],vacc5,state_index,state_names,epiweek_date)\n",
    "    \n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/vaccine_survey.csv\")\n",
    "    #'''\n",
    "    return week_cases,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_delphi_fb_google_survey(state_index,epiweek_date,start_week,end_week):\n",
    "    #fb_res_cli = Epidata.covidcast('fb-survey', 'raw_cli', 'day', 'state', [start_week, Epidata.range(start_week, end_week)], '*')\n",
    "    fb_res_cli1 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [start_week, Epidata.range(start_week, 20200601)], '*')\n",
    "    fb_res_cli2 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20200601, Epidata.range(20200601, 20200701)], '*')\n",
    "    fb_res_cli3 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20200701, Epidata.range(20200701, 20200901)], '*')\n",
    "    fb_res_cli4 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20200901, Epidata.range(20200901, 20201101)], '*')\n",
    "    fb_res_cli5 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20201101, Epidata.range(20201101, 20210101)], '*')\n",
    "    fb_res_cli6 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20210101, Epidata.range(20210101, 20210301)], '*')\n",
    "    fb_res_cli7 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20210301, Epidata.range(20210301, 20210501)], '*')\n",
    "    fb_res_cli8 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20210501, Epidata.range(20210501, 20210701)], '*')\n",
    "    fb_res_cli9 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20210701, Epidata.range(20210701, 20210901)], '*')\n",
    "    fb_res_cli10 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20210901, Epidata.range(20210901, 20211101)], '*')\n",
    "    fb_res_cli11 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20211101, Epidata.range(20211101, 20220101)], '*')\n",
    "    fb_res_cli12 = Epidata.covidcast('fb-survey', 'raw_wcli', 'day', 'state', [20220101, Epidata.range(20220101, end_week)], '*')\n",
    "\n",
    "\n",
    "    #'''\n",
    "    fb_cli1=fb_res_cli1['epidata']+fb_res_cli2['epidata']\n",
    "    fb_cli2=fb_cli1+fb_res_cli3['epidata']\n",
    "    fb_cli3=fb_cli2+fb_res_cli4['epidata']\n",
    "    #fb_res_cli=fb_cli3+fb_res_cli5['epidata']\n",
    "    fb_cli4=fb_cli3+fb_res_cli5['epidata'] \n",
    "    fb_cli5=fb_cli4+fb_res_cli6['epidata']\n",
    "    fb_cli6=fb_cli5+fb_res_cli7['epidata']\n",
    "    fb_cli7=fb_cli6+fb_res_cli8['epidata']\n",
    "    fb_cli8 = fb_cli7 + fb_res_cli9['epidata']\n",
    "    fb_cli9=fb_cli8 + fb_res_cli10['epidata']\n",
    "    fb_res_cli=fb_cli9+fb_res_cli11['epidata']+fb_res_cli12['epidata']\n",
    "    #print(fb_res1['epidata'][0])\n",
    "    #'''\n",
    "    google_res_cli = Epidata.covidcast('google-survey', 'raw_cli', 'day', 'state', [start_week, Epidata.range(start_week, end_week)], '*')\n",
    "    \n",
    "    #fb_res_wli = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [start_week, Epidata.range(start_week, end_week)], '*')\n",
    "    fb_res_wli1 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [start_week, Epidata.range(start_week, 20200601)], '*')\n",
    "    fb_res_wli2 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20200601, Epidata.range(20200601, 20200701)], '*')\n",
    "    fb_res_wli3 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20200701, Epidata.range(20200701, 20200901)], '*')\n",
    "    fb_res_wli4 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20200901, Epidata.range(20200901, 20201101)], '*')\n",
    "    fb_res_wli5 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20201101, Epidata.range(20201101, 20210101)], '*')\n",
    "    fb_res_wli6 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20210101, Epidata.range(20210101, 20210301)], '*')\n",
    "    fb_res_wli7 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20210301, Epidata.range(20210301, 20210501)], '*')\n",
    "    fb_res_wli8 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20210501, Epidata.range(20210501, 20210701)], '*')\n",
    "    fb_res_wli9 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20210701, Epidata.range(20210701, 20210901)], '*')\n",
    "    fb_res_wli10 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20210901, Epidata.range(20210901, 20211101)], '*')\n",
    "    fb_res_wli11 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20211101, Epidata.range(20211101, 20220101)], '*')\n",
    "    fb_res_wli12 = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20220101, Epidata.range(20220101, end_week)], '*')\n",
    "\n",
    "\n",
    "    #'''\n",
    "    fb_wli1=fb_res_wli1['epidata']+fb_res_wli2['epidata']\n",
    "    fb_wli2=fb_wli1+fb_res_wli3['epidata']\n",
    "    fb_wli3=fb_wli2+fb_res_wli4['epidata']\n",
    "    fb_wli4=fb_wli3+fb_res_wli5['epidata']\n",
    "    fb_wli5=fb_wli4+fb_res_wli6['epidata']\n",
    "    fb_wli6=fb_wli5+fb_res_wli7['epidata']\n",
    "    fb_wli7=fb_wli6+fb_res_wli8['epidata']\n",
    "    fb_wli8=fb_wli7+fb_res_wli9['epidata']\n",
    "    fb_wli9=fb_wli8+fb_res_wli10['epidata']\n",
    "    #fb_res_wli=fb_wli3+fb_res_wli5['epidata']\n",
    "    fb_res_wli=fb_wli9+fb_res_wli11['epidata']+fb_res_wli12['epidata']\n",
    "    \n",
    "    #google_res_wli = Epidata.covidcast('google-survey', 'raw_wili', 'day', 'state', [start_week, Epidata.range(start_week, end_week)], '*')\n",
    "    \n",
    "    #print('fb_cli1',fb_res_cli1['result'], fb_res_cli1['message'], len(fb_res_cli1['epidata']))\n",
    "    #print('fb_cli2',fb_res_cli2['result'], fb_res_cli2['message'], len(fb_res_cli2['epidata']))\n",
    "    #print('fb_cli3',fb_res_cli3['result'], fb_res_cli3['message'], len(fb_res_cli3['epidata']))\n",
    "    print('fb_wcli4',fb_res_cli4['result'], fb_res_cli4['message'], len(fb_res_cli4['epidata']))\n",
    "    print('fb_wcli5',fb_res_cli9['result'], fb_res_cli9['message'], len(fb_res_cli9s['epidata']))\n",
    "    print('fb_wcli6',fb_res_cli10['result'], fb_res_cli10['message'], len(fb_res_cli10['epidata']))\n",
    "    print('fb_wcli8',fb_res_cli11['result'], fb_res_cli11['message'], len(fb_res_cli11['epidata']))\n",
    "    print('fb_wcli9',fb_res_cli12['result'], fb_res_cli12['message'], len(fb_res_cli12['epidata']))\n",
    "    \n",
    "    print('google_cli',google_res_cli['result'], google_res_cli['message'], len(google_res_cli['epidata']))\n",
    "    \n",
    "    #print(fb_res_wli['result'], fb_res_wli['message'], len(fb_res_wli['epidata']))\n",
    "    \n",
    "    #print('fb_wili1',fb_res_wli1['result'], fb_res_wli1['message'], len(fb_res_wli1['epidata']))\n",
    "    #print('fb_wili2',fb_res_wli2['result'], fb_res_wli2['message'], len(fb_res_wli2['epidata']))\n",
    "    #print('fb_wili3',fb_res_wli3['result'], fb_res_wli3['message'], len(fb_res_wli3['epidata']))\n",
    "    print('fb_wili4',fb_res_wli4['result'], fb_res_wli4['message'], len(fb_res_wli4['epidata']))\n",
    "    print('fb_wili5',fb_res_wli5['result'], fb_res_wli5['message'], len(fb_res_wli5['epidata']))\n",
    "    print('fb_wili10',fb_res_wli10['result'], fb_res_wli10['message'], len(fb_res_wli10['epidata']))\n",
    "    print('fb_wili11',fb_res_wli11['result'], fb_res_wli11['message'], len(fb_res_wli11['epidata']))\n",
    "    print('fb_wili12',fb_res_wli12['result'], fb_res_wli12['message'], len(fb_res_wli12['epidata']))\n",
    "    \n",
    "    print('fb_wcli len',len(fb_res_cli))\n",
    "    print('fb_wli len',len(fb_res_wli))\n",
    "    #'''\n",
    "    state_names=list(state_index.keys())\n",
    "    cols=['fb_survey_wcli','google_survey_cli','fb_survey_wili']\n",
    "    week_cases={}\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    #'''\n",
    "    #week_cases[cols[0]]=read_survey_epidata(cols[0],fb_res_cli['epidata'],state_index,state_names,epiweek_date)\n",
    "    week_cases[cols[0]]=read_survey_epidata(cols[0],fb_res_cli,state_index,state_names,epiweek_date)\n",
    "    week_cases[cols[1]]=read_survey_epidata(cols[1],google_res_cli['epidata'],state_index,state_names,epiweek_date)\n",
    "    #week_cases[cols[2]]=read_survey_epidata(cols[2],fb_res_wli['epidata'],state_index,state_names,epiweek_date)\n",
    "    week_cases[cols[2]]=read_survey_epidata(cols[2],fb_res_wli,state_index,state_names,epiweek_date)\n",
    "    #'''\n",
    "\n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/fb-google-survey.csv\")\n",
    "    return week_cases,cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fips_code(state_names,fips_path,abbv_to_code=True):\n",
    "    #if abbv_to_code=True; read fips with given state code (state_names)\n",
    "    #if abbv_to_code=False; read state code with given state name (state_names)\n",
    "    \n",
    "    fips=pd.read_csv(fips_path+\"other_data/fips_codes.csv\",delimiter=',',dtype={'state_code':str})\n",
    "    if abbv_to_code:\n",
    "        fips=fips[['state','state_code']].drop_duplicates().reset_index()\n",
    "    else:\n",
    "        fips=fips[['state','state_name']].drop_duplicates().reset_index()\n",
    "    \n",
    "    state_fips={}\n",
    "    for name in state_names:\n",
    "        if abbv_to_code:\n",
    "            if name=='X':\n",
    "                row='US'\n",
    "            else:\n",
    "                row= str(fips.loc[fips['state'] == name,'state_code'].iloc[0])   \n",
    "        else:\n",
    "            if name=='X':\n",
    "                row='X'\n",
    "            else:\n",
    "                row= fips.loc[fips['state_name'] == name,'state'].iloc[0]   \n",
    "        \n",
    "        state_fips[name]=row\n",
    "    \n",
    "    #print(state_fips)\n",
    "    return state_fips\n",
    "'''\n",
    "def read_covidnet(data_covidnet,week_len,start,end,step,weekly_rate,region): \n",
    "    covid=np.empty(week_len)\n",
    "    covid[:]=np.nan\n",
    "    data_f=data_covidnet.loc[data_covidnet['CATCHMENT']==region]\n",
    "    #data_f=data[data['AGE CATEGORY']=='Overall'].reset_index()\n",
    "    for index,row in data_f.iterrows():\n",
    "        mmr_week=int(row['MMWR-WEEK'])\n",
    "        year=int(row['MMWR-YEAR'])\n",
    "        if row['AGE CATEGORY']=='Overall' and row['SEX']=='Overall' and row['RACE']=='Overall':\n",
    "            if year==2020:\n",
    "                if mmr_week>=start and mmr_week<=53:\n",
    "                    if region=='Entire Network':\n",
    "                        if row['NETWORK']=='COVID-NET':\n",
    "                            covid[mmr_week-10+step]=float(row[weekly_rate])\n",
    "                    else:\n",
    "                        covid[mmr_week-10+step]=float(row[weekly_rate])\n",
    "            elif year==2021:\n",
    "                if mmr_week<=end:\n",
    "                    if region=='Entire Network':\n",
    "                        if row['NETWORK']=='COVID-NET':\n",
    "                            covid[43+step+mmr_week]=float(row[weekly_rate])\n",
    "                    else:\n",
    "                        covid[43+step+mmr_week]=float(row[weekly_rate])\n",
    "    return covid\n",
    "'''\n",
    "def get_current_week(week,cur_date,last_date,strsplit='/'):\n",
    "    if strsplit=='/':\n",
    "        month,date,year=cur_date.split('/')\n",
    "    elif strsplit=='-':\n",
    "        year,month,date=cur_date.split('-')\n",
    "    cdate=int(date)\n",
    "    cmonth=int(month)\n",
    "    year=int(year)\n",
    "    \n",
    "    #print(cdate,cmonth,year)\n",
    "    if year==20 or year==21 or year == 22:\n",
    "        stryear='20'+str(year)\n",
    "        year=int(stryear)\n",
    "        \n",
    "    for id in range(0,len(week)):\n",
    "        y,m,d=week[id].split('-')\n",
    "        wd,wm,wy=int(d),int(m),int(y)\n",
    "        if wm==cmonth and wd==cdate and wy==year:\n",
    "              return id\n",
    "    if strsplit=='/':\n",
    "        m,d,y=last_date.split('/')\n",
    "    elif strsplit=='-':\n",
    "        y,m,d=last_date.split('-')\n",
    "    \n",
    "    wd,wm,wy=int(d),int(m),int(y)\n",
    "    if wm==cmonth and cdate==wd and wy==year:\n",
    "        return len(week)-1\n",
    "    #print('week index not found:'+cur_date)\n",
    "    return -1\n",
    "\n",
    "def find_same_week(week,cur_date,last_date,date_string=True):\n",
    "    #date,month,year=cur_date.split('-')\n",
    "    tmp_week=week\n",
    "    #tmp_week[-1]=last_date\n",
    "    if date_string:\n",
    "        year,month,date=cur_date[:4],cur_date[4:6],cur_date[6:8]\n",
    "        ly,lm,ld=last_date[:4],last_date[4:6],last_date[6:8]\n",
    "    else:\n",
    "        year,month,date=cur_date.split('-')\n",
    "        ly,lm,ld=last_date.split('-')\n",
    "        \n",
    "    cdate=int(date)\n",
    "    cmonth=int(month)\n",
    "    year=int(year)\n",
    "    \n",
    "    if year==20 or year==21 or year == 22:\n",
    "        stryear='20'+str(year)\n",
    "        year=int(stryear)\n",
    "\n",
    "    for id in range(0,len(tmp_week)):\n",
    "        y,m,d=tmp_week[id].split('-')\n",
    "        wd,wm,wy=int(d),int(m),int(y)\n",
    "        if wm==cmonth and cdate==wd and wy==year:\n",
    "            return id\n",
    "    \n",
    "    ldd,lmm,lyy=int(ld),int(lm),int(ly)\n",
    "    if lmm==cmonth and cdate==ldd and lyy==year:\n",
    "        return len(week)-1\n",
    "    #print('week index not found:'+cur_date)\n",
    "    return -1\n",
    "\n",
    "def find_week_index(week,cur_date,date_string=True,strsplit='-'):\n",
    "    if date_string:\n",
    "        year,month,date=cur_date[:4],cur_date[4:6],cur_date[6:8]\n",
    "    else:\n",
    "        if strsplit=='-':\n",
    "            year,month,date=cur_date.split(strsplit)\n",
    "        elif strsplit=='/':\n",
    "            month,date,year=cur_date.split(strsplit)\n",
    "    \n",
    "    cdate=int(date)\n",
    "    cmonth=int(month)\n",
    "    year=int(year)\n",
    "    \n",
    "    if year==20 or year==21 or year == 22:\n",
    "        stryear='20'+str(year)\n",
    "        year=int(stryear)\n",
    "        \n",
    "    for id in range(0,len(week)):\n",
    "        y,m,d=week[id].split('-')\n",
    "        wd,wm,wy=int(d),int(m),int(y)\n",
    "        if wm==cmonth:\n",
    "          #if wm==12:\n",
    "           #   print(wm,wd,wy,id+1,len(week))\n",
    "          if cdate>wd and wy==year and (id+1)<len(week):\n",
    "              yn,mn,dn=week[id+1].split('-')\n",
    "              wmn=int(mn)\n",
    "              if wmn==(cmonth%12)+1:\n",
    "                  return id+1\n",
    "          elif cdate<=wd and wy==year:\n",
    "              return id\n",
    "    print('week index not found:'+cur_date)\n",
    "    print(cdate,cmonth,year)\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_apple_mobility_per_date(inputdir,epiweek_date,state_index,dic_names_to_abbv,start_week,end_week):\n",
    "    data=pd.read_csv(inputdir+\"applemobilitytrends.csv\",low_memory=False)\n",
    "    state_names=list(dic_names_to_abbv.keys())\n",
    "    dates=list(data.columns)\n",
    "    dates=dates[6:]\n",
    "    data = data.loc[data['region'].isin(state_names)]\n",
    "    data=data.fillna(0)\n",
    "    #print(data.shape)\n",
    "    date_dic={dates[i]: i for i in range(len(dates))} \n",
    "    week_cases=np.zeros((len(state_names),len(dates)))\n",
    "    for ix,row in data.iterrows():\n",
    "        if row['transportation_type']=='driving':\n",
    "            if row['region'] in state_names:\n",
    "                state_id=state_index[dic_names_to_abbv[row['region']]] \n",
    "                for d in dates:\n",
    "                    #w_idx=find_week_index(epiweek_date,d,date_string=False)\n",
    "                    #if w_idx!=-1:\n",
    "                    week_cases[state_id][date_dic[d]]+=float(row[d])\n",
    "    apple_dic={}\n",
    "    apple_dic['mobility']=week_cases    \n",
    "    \n",
    "    unit_test(apple_dic,['mobility'],dates,state_index,\"unit_test/mobility-sampled-data.csv\")\n",
    "    \n",
    "    return apple_dic,['apple_mobility']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epiweek_list(week_start,week_end,year):\n",
    "    def convert(x):\n",
    "        return Week.fromstring(str(x))\n",
    "    \n",
    "    wstart=convert(week_start)\n",
    "    wend=convert(week_end)\n",
    "    #print(wstart,wend)\n",
    "    week_edate_list=[]\n",
    "    week_list=[]\n",
    "    for week in Year(year).iterweeks():\n",
    "        date_time = week.enddate().strftime(\"%Y-%m-%d\")\n",
    "        #print(week,date_time)\n",
    "        if week>=wstart and week<=wend:\n",
    "            week_edate_list.append(date_time)\n",
    "            week_list.append(str(week))\n",
    "     \n",
    "    return week_list,week_edate_list  \n",
    "\n",
    "\n",
    "def read_mobility(inputdir,epiweek_date,end_week,MISSING_TOKEN=0):\n",
    "    data=pd.read_csv(inputdir+\"Global_Mobility_Report.csv\",low_memory=False)\n",
    "    data=data[data['country_region_code']=='US']\n",
    "    data=data.drop(data[data['sub_region_1'] == 'Hawaii'].index)\n",
    "    data=data.drop(columns=['sub_region_2'])\n",
    "\n",
    "    state_names=data['sub_region_1'].drop_duplicates().values\n",
    "    state_names[0]='X' #changing nan to US national X\n",
    "    cols=data.columns\n",
    "    cols=list(cols[8:])\n",
    "    dic_names_to_abbv=read_fips_code(state_names,inputdir,abbv_to_code=False)\n",
    "    \n",
    "    state_index = {dic_names_to_abbv[state_names[i]]: i for i in range(len(state_names))} \n",
    "    #data[cols] = data[cols].fillna(MISSING_TOKEN)\n",
    "    num_states=len(state_names)\n",
    "    #print(cols)\n",
    "    week_cases={}\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.empty((num_states,len(epiweek_date)))\n",
    "        week_cases[c][:][:]=np.nan\n",
    "        '''\n",
    "        week_cases[c]=np.zeros((num_states,len(epiweek_date)))\n",
    "        if (len(epiweek_date)-end_week)!=0:\n",
    "            week_cases[c][:][end_week]=np.nan\n",
    "        '''\n",
    "    print(cols)\n",
    "    #'''\n",
    "    for ix,row in data.iterrows():\n",
    "        if type(row['sub_region_1']) is float:\n",
    "            state_id=0\n",
    "        else:\n",
    "            state_id=state_index[dic_names_to_abbv[row['sub_region_1']]]\n",
    "        week_id=find_week_index(epiweek_date,str(row['date']),date_string=False)\n",
    "        if week_id!=-1:\n",
    "            for c in cols:\n",
    "                if pd.isnull(row[c])==False:\n",
    "                    if np.isnan(week_cases[c][state_id][week_id]):\n",
    "                            week_cases[c][state_id][week_id]=0\n",
    "                    week_cases[c][state_id][week_id]+=row[c]\n",
    "    \n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/google-mobility.csv\")\n",
    "    #'''\n",
    "    \n",
    "    return week_cases,cols,state_index,dic_names_to_abbv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vaccine_doses(inputdir,epiweek_date,state_index,dic_names_to_abbv,last_date):\n",
    "    data=pd.read_csv(inputdir+\"vaccine_data_us_state_timeline.csv\",low_memory=False)\n",
    "    #data=data.fillna(0)\n",
    "    #state_names=list(state_index.keys())\n",
    "    state_names=list(dic_names_to_abbv.keys())\n",
    "    #cols=['people_total','people_total_2nd_dose']\n",
    "    cols=['Stage_One_Doses','Stage_Two_Doses']\n",
    "    week_cases={}\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    \n",
    "    for ix,row in data.iterrows():\n",
    "        #print(len(epiweek_date),row['Date'],last_date)\n",
    "        w_idx=get_current_week(epiweek_date,row['Date'],last_date,strsplit='-')\n",
    "        if row['Province_State'] in state_names and w_idx!=-1 and row['Vaccine_Type']=='All':\n",
    "            #state_id=state_index[row['stabbr']] \n",
    "            state_id=state_index[dic_names_to_abbv[row['Province_State']]] \n",
    "            for c in cols:\n",
    "                val=None\n",
    "                if pd.isnull(row[c])==False:\n",
    "                    val=float(row[c])\n",
    "                elif w_idx>0:\n",
    "                    val=week_cases[c][state_id][w_idx-1]               \n",
    "                week_cases[c][state_id][w_idx]=val\n",
    "                week_cases[c][0][w_idx]+=val\n",
    "    \n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/vaccine.csv\")\n",
    "    \n",
    "    return week_cases,cols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_apple_mobility(inputdir,epiweek_date,state_index,dic_names_to_abbv,start_week,end_week):\n",
    "    data=pd.read_csv(inputdir+\"applemobilitytrends.csv\",low_memory=False)\n",
    "    state_names=list(dic_names_to_abbv.keys())\n",
    "    dates=list(data.columns)\n",
    "    dates=dates[6:]\n",
    "    data = data.loc[data['region'].isin(state_names)]\n",
    "    #data=data.fillna(0)\n",
    "    #print(data.shape)\n",
    "    #week_cases=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    week_cases=np.empty((len(state_names),len(epiweek_date)))\n",
    "    week_cases[:][:]=np.nan\n",
    "    for ix,row in data.iterrows():\n",
    "        if row['transportation_type']=='driving':\n",
    "            if row['region'] in state_names:\n",
    "                state_id=state_index[dic_names_to_abbv[row['region']]] \n",
    "                for d in dates:\n",
    "                    w_idx=find_week_index(epiweek_date,d,date_string=False)\n",
    "                    if w_idx!=-1 and pd.isnull(row[d])==False:\n",
    "                        if np.isnan(week_cases[state_id][w_idx]):\n",
    "                            week_cases[state_id][w_idx]=0\n",
    "                        week_cases[state_id][w_idx]+=float(row[d])\n",
    "    apple_dic={}\n",
    "    apple_dic['apple_mobility']=week_cases    \n",
    "    \n",
    "    unit_test(apple_dic,['apple_mobility'],epiweek_date,state_index,\"unit_test/apple.csv\")\n",
    "    \n",
    "    return apple_dic,['apple_mobility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dex(inputdir,epiweek_date,state_index,start_week,end_week):\n",
    "    data=pd.read_csv(inputdir+\"state_dex.csv\",low_memory=False)\n",
    "    cols=[]\n",
    "    state_names=list(state_index.keys())\n",
    "    for c in data.columns:\n",
    "        if 'dex' in c:\n",
    "            cols.append(c)\n",
    "    \n",
    "    week_cases={}\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.empty((len(state_names),len(epiweek_date)))\n",
    "        week_cases[c][:][:]=np.nan\n",
    "        week_cases[c][0][:]=0\n",
    "        '''\n",
    "        week_cases[c]=np.zeros((len(state_names),len(epiweek_date)))\n",
    "        if (end_week-len(epiweek_date))!=0:\n",
    "            week_cases[c][:][-1]=np.nan\n",
    "        '''\n",
    "    \n",
    "    for ix,row in data.iterrows():\n",
    "        w_idx=find_week_index(epiweek_date,row['date'],date_string=False)\n",
    "        if row['state'] in state_names and w_idx!=-1:\n",
    "            state_id=state_index[row['state']]\n",
    "            for c in cols:\n",
    "                if not math.isnan(row[c]):\n",
    "                    if np.isnan(week_cases[c][state_id][w_idx]):\n",
    "                        week_cases[c][state_id][w_idx]=0\n",
    "                    week_cases[c][state_id][w_idx]+=float(row[c])\n",
    "                    week_cases[c][0][w_idx]+=float(row[c])\n",
    "    \n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/dex.csv\")\n",
    "    \n",
    "    return week_cases,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_emergency(inputdir,epiweek_date,state_index,start_week,end_week):\n",
    "    #data=pd.read_csv(inputdir+\"emergency-visits.csv\")\n",
    "    #data=pd.read_csv(inputdir+\"covid-like-illness.csv\")\n",
    "    data=pd.read_csv(inputdir+\"covid-like-illness-v202040.csv\")\n",
    "    cols=['Number of Facilities Reporting','CLI Percent of Total Visits']\n",
    "    print(data.columns)\n",
    "    data=data[data['Week']>=start_week]\n",
    "    state_names=list(state_index.keys())\n",
    "    week_cases={}\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.full([len(state_names),len(epiweek_date)],np.nan)\n",
    "    \n",
    "    \n",
    "    region_map={'X':0,'Region 1':1,'Region 2':2, 'Region 3':3,'Region 4':4,'Region 5':5,'Region 6':6,\n",
    "               'Region 7':7,'Region 8':8,'Region 9':9,'Region 10':10}\n",
    "\n",
    "    file = open(inputdir+\"other_data/hhs_regions_abbv.txt\", 'r') \n",
    "    Lines = file.readlines() \n",
    "    state_hhs_map={}\n",
    "    #### mapping each state to a hhs region ###\n",
    "    state_hhs_map[0]=0 # setting national value\n",
    "    for line in Lines:\n",
    "        regions=line.strip().split(',')\n",
    "        reg_id=int(regions[0])\n",
    "        for j in range(1,len(regions)):\n",
    "            if state_index.get(regions[j],-1)!=-1:\n",
    "                state_id=state_index[regions[j]]\n",
    "                state_hhs_map[state_id]=reg_id\n",
    "            else:\n",
    "                print('state not found '+regions[j])\n",
    "    print(state_index)\n",
    "    list1 = state_hhs_map.keys()\n",
    "    list2 = sorted(list1)\n",
    "    print(list2)\n",
    "    for ix,row in data.iterrows():\n",
    "        reg_id=region_map[row['region']]\n",
    "        week=str(row['Week'])\n",
    "        w_idx=int(week[4:])-1\n",
    "        year=int(week[:4])\n",
    "        if year>2020:\n",
    "            w_idx+=53*(year-2020)\n",
    "        for st in range(len(state_names)):\n",
    "            if state_hhs_map[st]==reg_id:\n",
    "                for c in cols:\n",
    "                    reporting=row[c]\n",
    "                    if c=='Number of Facilities Reporting':\n",
    "                        reporting=int(reporting.replace(',',''))\n",
    "                    #print(state_names[st],w_idx,reporting)\n",
    "                    week_cases[c][st][w_idx]=reporting\n",
    "    \n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/emergency.csv\")\n",
    "    return week_cases,cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kinsa(inputdir,epiweek_date,state_index,start_week,end_week,isregion=False):\n",
    "    if isregion:\n",
    "        data=pd.read_csv(inputdir+\"kinsa_observedili_region_avg.csv\")\n",
    "    else:\n",
    "        data=pd.read_csv(inputdir+\"kinsa_observedili_state_avg.csv\")\n",
    "    state_names=list(state_index.keys())\n",
    "    week_cases=np.full([len(state_names),len(epiweek_date)],np.nan)\n",
    "    grouped_kinsa= data.groupby(['region'])\n",
    "    \n",
    "    for name, group in grouped_kinsa:\n",
    "        state_id=state_index[name]\n",
    "        #print(name)\n",
    "        #print(group)\n",
    "        week_cases[state_id][:end_week]=group['cases'][start_week-1:end_week]\n",
    "    kinsa_dic={}\n",
    "    kinsa_dic['kinsa_cases']=week_cases\n",
    "    unit_test(kinsa_dic,['kinsa_cases'],epiweek_date,state_index,\"unit_test/kinsa.csv\")\n",
    "    return kinsa_dic,['kinsa_cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iqvia(inputdir,epiweek_date,state_index,start_week,end_week):\n",
    "    data=pd.read_csv(inputdir+\"iqvia_Processed.csv\")\n",
    "    state_names=list(state_index.keys())\n",
    "    week_cases={}\n",
    "    grouped_iqvia= data.groupby(['region'])\n",
    "    cols=list(data.columns)\n",
    "    cols=cols[4:]\n",
    "    print(cols)\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.full([len(state_names),len(epiweek_date)],np.nan)\n",
    "    \n",
    "    for name, group in grouped_iqvia:\n",
    "        state_id=state_index[name]\n",
    "        for c in cols:\n",
    "            #week_cases[c][state_id][start_week-10:end_week-start_week]=group[c]\n",
    "            week_cases[c][state_id][:end_week]=group[c][start_week-1:end_week]\n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/iqvia.csv\")\n",
    "    return week_cases,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cdc_hosp(inputdir,epiweek_date,state_index,end_week):\n",
    "    data=pd.read_csv(inputdir+\"COVID-19_Reported_Timeseries.csv\")\n",
    "    #data=pd.read_csv(inputdir+\"reported_hospital_timeseries.csv\")\n",
    "    \n",
    "    #data=data.fillna(0)\n",
    "    state_index['PR'] = 51\n",
    "    state_index['VI'] = 52\n",
    "    state_names=list(state_index.keys())\n",
    "    week_cases={}\n",
    "    flu_week_cases={}\n",
    "    #cols_d=list(data.columns)\n",
    "    #print(cols_d)\n",
    "    #cols=cols[2:]\n",
    "    cols=['cdc_hospitalized']\n",
    "    flu_cols=['cdc_flu_hosp']\n",
    "    \n",
    "    cols_to_check=['previous_day_admission_adult_covid_confirmed', 'previous_day_admission_pediatric_covid_confirmed']\n",
    "    flu_cols_to_check=['previous_day_admission_influenza_confirmed']\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.empty((len(state_names),len(epiweek_date)))\n",
    "    week_cases[c][:][:]=np.nan\n",
    "    week_cases[c][0][:]=0\n",
    "    for c in flu_cols:\n",
    "        flu_week_cases[c]=np.empty((len(state_names),len(epiweek_date)))\n",
    "    flu_week_cases[c][:][:]=np.nan\n",
    "    flu_week_cases[c][0][:]=0\n",
    "    \n",
    "    # shift one up\n",
    "    data['datetime'] = pd.to_datetime(data['date'])\n",
    "    data = data.sort_values(by=['datetime'])\n",
    "    data['previous_day_admission_influenza_confirmed'] = data.groupby('state')['previous_day_admission_influenza_confirmed'].shift(-1)\n",
    "    data.reset_index()\n",
    "    \n",
    "    # data['previous_day_admission_influenza_confirmed'] = data.groupby('state')['previous_day_admission_influenza_confirmed'].shift(-1)\n",
    "    # data['previous_day_admission_influenza_confirmed'] = data['previous_day_admission_influenza_confirmed'].shift(-1)\n",
    "\n",
    "    for index,row in data.iterrows():\n",
    "        if row['state'] in state_index.keys():\n",
    "            state_id=state_index[row['state']]\n",
    "        else:\n",
    "            continue\n",
    "        date=str(row['date'])\n",
    "        week_id=find_week_index(epiweek_date,date.replace('/','-'),date_string=False)\n",
    "        # flu_date = str(datetime.strptime(date.replace('/','-'), '%Y-%m-%d') - timedelta(days=1))\n",
    "        # flu_date = flu_date[:10]\n",
    "        # flu_week_id = find_week_index(epiweek_date, flu_date, date_string=False)\n",
    "        if week_id!=-1:\n",
    "            ttl=0\n",
    "            flag_is_not_nan=False\n",
    "            for c in cols_to_check: \n",
    "                if pd.isnull(row[c])==False:\n",
    "                    ttl+=float(row[c])\n",
    "                    flag_is_not_nan=True\n",
    "            if flag_is_not_nan:\n",
    "                if np.isnan(week_cases[cols[0]][state_id][week_id]):\n",
    "                    week_cases[cols[0]][state_id][week_id]=0\n",
    "                week_cases[cols[0]][state_id][week_id]+=ttl\n",
    "                week_cases[cols[0]][0][week_id]+=ttl\n",
    "            else:\n",
    "                week_cases[cols[0]][state_id][week_id]=np.nan\n",
    "                \n",
    "            # same for flu hosp\n",
    "            ttl=0\n",
    "            flag_is_not_nan=False\n",
    "            for c in flu_cols_to_check: \n",
    "                if pd.isnull(row[c])==False:\n",
    "                    ttl+=float(row[c])\n",
    "                    flag_is_not_nan=True\n",
    "            if flag_is_not_nan:\n",
    "                if np.isnan(flu_week_cases[flu_cols[0]][state_id][week_id]):\n",
    "                    flu_week_cases[flu_cols[0]][state_id][week_id]=0\n",
    "                flu_week_cases[flu_cols[0]][state_id][week_id]+=ttl\n",
    "                flu_week_cases[flu_cols[0]][0][week_id]+=ttl\n",
    "            else:\n",
    "                flu_week_cases[flu_cols[0]][state_id][week_id]+=0\n",
    "                \n",
    "            \n",
    "                    \n",
    "    '''\n",
    "    week_cases[c][0]=np.zeros(len(epiweek_date))\n",
    "    for s in range(1,len(state_names)):\n",
    "        for c in cols:\n",
    "            week_cases[c][0]+=week_cases[c][s]\n",
    "            #week_cases[c][s][-1]=np.nan #considering data 2 weeks lag\n",
    "            #week_cases[c][s][-2]=np.nan\n",
    "    '''\n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/cdc_hosp.csv\") \n",
    "    del state_index['PR']\n",
    "    del state_index['VI']\n",
    "    return week_cases,cols,flu_week_cases,flu_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_covidnet(data_covidnet,week_len,start,end,step,weekly_rate,region): #region='X'\n",
    "    print(region)\n",
    "    covid=np.empty(week_len)\n",
    "    covid[:]=np.nan\n",
    "    data_f=data_covidnet.loc[data_covidnet['CATCHMENT']==region]\n",
    "    #data_f=data[data['AGE CATEGORY']=='Overall'].reset_index()\n",
    "    for index,row in data_f.iterrows():\n",
    "        mmr_week=int(row['MMWR-WEEK'])\n",
    "        year=int(row['MMWR-YEAR'])\n",
    "        if row['AGE CATEGORY']=='Overall' and row['SEX']=='Overall' and row['RACE']=='Overall':\n",
    "            if year==2020:\n",
    "                if mmr_week>=start and mmr_week<=53:\n",
    "                    if region=='Entire Network':\n",
    "                        if row['NETWORK']=='COVID-NET':\n",
    "                            covid[mmr_week-10+step]=float(row[weekly_rate])\n",
    "                    else:\n",
    "                        covid[mmr_week-10+step]=float(row[weekly_rate])\n",
    "            elif year==2021:\n",
    "                if mmr_week<=52:\n",
    "                    if region=='Entire Network':\n",
    "                        if row['NETWORK']=='COVID-NET':\n",
    "                            covid[43+step+mmr_week]=float(row[weekly_rate])\n",
    "                    else:\n",
    "                        covid[43+step+mmr_week]=float(row[weekly_rate])\n",
    "            elif year==2022:\n",
    "                if mmr_week<=end:\n",
    "                    if region=='Entire Network':\n",
    "                        if row['NETWORK']=='COVID-NET':\n",
    "                            covid[95+step+mmr_week]=float(row[weekly_rate])\n",
    "                    else:\n",
    "                        covid[95+step+mmr_week]=float(row[weekly_rate])\n",
    "                \n",
    "    return covid\n",
    "\n",
    "def read_covidnet_data(inputdir,epiweek_date,state_index,dic_names_to_abbv,start_week,end_week,step):\n",
    "    #data_covidnet=pd.read_csv(inputdir+\"COVID-NET_Processed.csv\",delimiter=',')\n",
    "    if date.today().weekday() != 6:\n",
    "        week_num = (date.today()-timedelta(days=2)).strftime(\"%U\")\n",
    "        year_week_num = \"2022\" + week_num\n",
    "    else:\n",
    "        week_num = (date.today()-timedelta(days=1)).strftime(\"%U\")\n",
    "        year_week_num = \"2022\" + week_num\n",
    "    file_name = \"COVID-NET_v\"+year_week_num + \".csv\"\n",
    "    data_covidnet=pd.read_csv(inputdir+file_name,delimiter=',')\n",
    "    columns=list(data_covidnet.columns)\n",
    "    dic_names_to_abbv['Entire Network']='X'\n",
    "    state_names=list(dic_names_to_abbv.keys())\n",
    "    #print(state_names)\n",
    "    #state_names=list(state_index.keys())\n",
    "    #print(columns)\n",
    "    weekly_rate='WEEKLY RATE'\n",
    "    if 'WEEKLY RATE' in columns:\n",
    "        data_covidnet=data_covidnet[['CATCHMENT','NETWORK','MMWR-YEAR','MMWR-WEEK','AGE CATEGORY','SEX','RACE','WEEKLY RATE']]\n",
    "    else:\n",
    "        data_covidnet=data_covidnet[['CATCHMENT','NETWORK','MMWR-YEAR','MMWR-WEEK','AGE CATEGORY','SEX','RACE','WEEKLY RATE ']]\n",
    "        weekly_rate='WEEKLY RATE '\n",
    "    \n",
    "    week_cases=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    #for st in state_index.keys():\n",
    "    for state in state_names:\n",
    "        if state=='United States' or state=='X':\n",
    "            continue\n",
    "        st=dic_names_to_abbv[state]\n",
    "        #print(st,state_index[st])\n",
    "        week_cases[state_index[st]][:]=read_covidnet(data_covidnet,len(epiweek_date),start_week,end_week,step,weekly_rate,region=state)\n",
    "    \n",
    "    covidnet_dic={}\n",
    "    covidnet_dic['covidnet']=week_cases\n",
    "    unit_test(covidnet_dic,['covidnet'],epiweek_date,state_index,\"unit_test/covidnet.csv\")\n",
    "    return covidnet_dic,['covidnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hosp_negative_total_data(inputdir,epiweek_date,state_index):\n",
    "    data_hosp=pd.read_csv(inputdir+\"COVID-19_PCR_Testing_Time_Series.csv\",delimiter=',')\n",
    "    state_names=list(state_index.keys())\n",
    "    week_cases={}\n",
    "    cols=['negativeIncr','total_resultsIncr']\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    \n",
    "    for index,row in data_hosp.iterrows():\n",
    "        if row['overall_outcome']=='Negative':\n",
    "            if row['state'] in state_index.keys():\n",
    "                state_id=state_index[row['state']]\n",
    "                date=row['date']\n",
    "                week_id=find_week_index(epiweek_date,date.replace('/','-'),date_string=False)\n",
    "#                 print(week_cases)[state_id][week_id]\n",
    "                week_cases[cols[0]][state_id][week_id]+=float(row['new_results_reported'])\n",
    "                week_cases[cols[1]][state_id][week_id]+=float(row['total_results_reported'])\n",
    "            \n",
    "                week_cases[cols[0]][0][week_id]+=float(row['new_results_reported'])\n",
    "                week_cases[cols[1]][0][week_id]+=float(row['total_results_reported'])\n",
    "    \n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/hosp_neg_ttl.csv\") \n",
    "    \n",
    "    return week_cases,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_columns(orig_col,epiweek_date):\n",
    "    d_len=len(epiweek_date)-3\n",
    "    imputed_col=np.empty(len(epiweek_date))\n",
    "    imputed_col[-2:]=orig_col[-2:]\n",
    "    while d_len>=0:\n",
    "        if orig_col[d_len]==0 or orig_col[d_len]==np.nan:     \n",
    "            if orig_col[d_len+2]!=0: #geometric mean a,b,c a=0,c!=0\n",
    "                imputed_col[d_len]=np.square(orig_col[d_len+1])/orig_col[d_len+2]\n",
    "            #else orig_col[d_len+1]!=0:: #arithmetic mean a=0, c==0\n",
    "             #   imputed_col[d_len]=2*orig_col[d_len+1]-orig_col[d_len+2]\n",
    "        d_len-=1\n",
    "    return imputed_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hosp_cases_nat_state(data_national,data_state,epiweek_date,week_save,state_index,cols,state_error,last_date):\n",
    "    state_names=list(state_index.keys())\n",
    "    week_cases={}\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    \n",
    "    #processing national cases\n",
    "    nat_id=state_index['X']\n",
    "    nat_hosp_incr=np.zeros(len(epiweek_date))\n",
    "    for index,row in data_national.iterrows():\n",
    "        week_id=find_week_index(week_save,str(row['date']))\n",
    "        for c in range(len(cols)): #if next1,next2 consider then do cols-4\n",
    "            week_cases[cols[c]][nat_id][week_id]+=row[cols[c]]\n",
    "    \n",
    "    #processing state cases\n",
    "    for index,row in data_state.iterrows():\n",
    "        state_id=state_index[row['states']]\n",
    "        week_id=find_week_index(week_save,str(row['date']))\n",
    "        if week_id!=-1:\n",
    "            for c in range(len(cols)): #if next1,next2 consider then do cols-4\n",
    "                if cols[c]=='hospitalizedIncrease':\n",
    "                    if row['states'] in state_error:\n",
    "                        week_id_for_hsp=find_same_week(epiweek_date,str(row['date']),last_date)\n",
    "                        if week_id_for_hsp!=-1:\n",
    "                            week_cases[cols[c]][state_id][week_id_for_hsp]=max(0,row['hospitalizedCurrently'])\n",
    "                    else:\n",
    "                        week_cases[cols[c]][state_id][week_id]+=max(0,row[cols[c]])\n",
    "                        nat_hosp_incr[week_id]+=max(0,row[cols[c]])\n",
    "                    #week_id_for_hsp=find_same_week(epiweek_date,str(row['date']),last_date)\n",
    "                    #if week_id_for_hsp!=-1:\n",
    "                     #   week_cases[cols[c]][state_id][week_id_for_hsp]=max(0,row['hospitalizedCurrently'])\n",
    "                elif cols[c]=='recovered':\n",
    "                    week_id_for_rec=find_same_week(epiweek_date,str(row['date']),last_date)\n",
    "                    if week_id_for_rec!=-1:\n",
    "                        week_cases[cols[c]][state_id][week_id_for_rec]=max(0,row[cols[c]])                           \n",
    "                else:\n",
    "                    week_cases[cols[c]][state_id][week_id]+=max(0,row[cols[c]])\n",
    "    \n",
    "    #for states with no hospitalze cumulative using formula week[t]=hosp_cur[t]-(week[t-1]/2)\n",
    "    \n",
    "    c='hospitalizedIncrease'\n",
    "    for s in state_error:\n",
    "    #for s in state_names:\n",
    "        st_idx=state_index[s]\n",
    "        hsp_incr=np.zeros(len(epiweek_date))\n",
    "        hsp_incr[0]=week_cases[c][st_idx][0]\n",
    "        nat_hosp_incr[0]+=week_cases[c][st_idx][0]\n",
    "        for w in range(1,len(epiweek_date)):\n",
    "            hsp_incr[w]=max(0,week_cases[c][st_idx][w]-float(hsp_incr[w-1]/2))\n",
    "            nat_hosp_incr[w]+=hsp_incr[w]\n",
    "            #if s=='CA':\n",
    "             #   print(w,week_cases[c][st_idx][w],hsp_incr[w-1],hsp_incr[w])\n",
    "        week_cases[c][st_idx]=hsp_incr\n",
    "    \n",
    "    c='recovered'\n",
    "    rec_nat=np.zeros(len(epiweek_date))\n",
    "    for s in state_names:\n",
    "        st_idx=state_index[s]\n",
    "        rec_incr=np.zeros(len(epiweek_date))\n",
    "        rec_incr[0]=week_cases[c][st_idx][0]\n",
    "        for w in range(1,len(epiweek_date)):\n",
    "            rec_incr[w]=week_cases[c][st_idx][w]-week_cases[c][st_idx][w-1]\n",
    "            rec_nat[w]+=max(rec_incr[w],0)\n",
    "        week_cases[c][st_idx]=rec_incr\n",
    "    \n",
    "    week_cases[c][nat_id]=rec_nat\n",
    "    \n",
    "    '''    \n",
    "    #for data imputation\n",
    "    for st in range(len(state_names)):\n",
    "        for c in range(0,len(cols)):#if next1,next2 consider then do cols-4\n",
    "            if c<3 or c>4: #avoiding inVentilation, inICU for imputation\n",
    "                imputed_week=impute_columns(week_cases[cols[c]][st],epiweek_date)\n",
    "                week_cases[cols[c]][st]=imputed_week\n",
    "    '''\n",
    "    '''\n",
    "    #filling up next1,next2 values\n",
    "    h_next1=np.empty((len(state_names),len(epiweek_date)))\n",
    "    h_next2=np.empty((len(state_names),len(epiweek_date)))\n",
    "    d_next1=np.empty((len(state_names),len(epiweek_date)))\n",
    "    d_next2=np.empty((len(state_names),len(epiweek_date)))\n",
    "    \n",
    "    h_next1[:,:]=np.nan\n",
    "    h_next2[:,:]=np.nan\n",
    "    d_next1[:,:]=np.nan\n",
    "    d_next2[:,:]=np.nan     \n",
    "    for st in range(len(state_names)):\n",
    "        for widx in range(0,len(epiweek_date)-1):\n",
    "            h_next1[st][widx]=week_cases['hospitalizedIncrease'][st][widx+1]\n",
    "            d_next1[st][widx]=week_cases['deathIncrease'][st][widx+1]\n",
    "            if widx+2<len(epiweek_date):\n",
    "                h_next2[st][widx]=week_cases['hospitalizedIncrease'][st][widx+2]\n",
    "                d_next2[st][widx]=week_cases['deathIncrease'][st][widx+2]\n",
    "        week_cases['h_next1'][st]=h_next1[st]\n",
    "        week_cases['h_next2'][st]=h_next2[st]\n",
    "        week_cases['d_next1'][st]=d_next1[st]\n",
    "        week_cases['d_next2'][st]=d_next2[st]\n",
    "    '''\n",
    "    '''\n",
    "    out=pd.DataFrame(columns=['date','states']+cols)\n",
    "    for st in state_index.keys():\n",
    "        tmp_out=pd.DataFrame(columns=['date','states']+cols)\n",
    "        tmp_out['date']=week_save\n",
    "        tmp_out['states']=[st]*len(epiweek_date)\n",
    "        for c in cols:\n",
    "            tmp_out[c]=week_cases[c][state_index[st]]\n",
    "        out=out.append(tmp_out,ignore_index=True)\n",
    "    \n",
    "    out.to_csv(\"/Users/anikat/Downloads/covid-hospitalization-data/hosp_check.csv\",index=False)\n",
    "    '''\n",
    "    np.savetxt(\"unit_test/hos_nat_aggregated.csv\", nat_hosp_incr, fmt='%.4f')\n",
    "    return week_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hospitalization(input_national,input_state,epiweek_date,week_save,state_index,\n",
    "                         last_date,state_error):\n",
    "    nat_data_path=\"us-daily-hospitalizations.csv\"\n",
    "    state_data_path=\"states-daily-hospitalizations.csv\"\n",
    "    nat_data=pd.read_csv(input_national+nat_data_path,delimiter=',')\n",
    "    state_data=pd.read_csv(input_state+state_data_path,delimiter=',')\n",
    "    state_data=state_data.rename(columns={\"state\": \"states\"})\n",
    "    \n",
    "    columns=['date','states','hospitalizedCurrently','onVentilatorCurrently','positiveIncrease','negativeIncrease',\n",
    "             'totalTestResultsIncrease','inIcuCurrently','recovered','deathIncrease','hospitalizedIncrease']\n",
    "    rows_to_drop=['GU','AS','HI','PR', 'MP','VI']\n",
    "    \n",
    "    nat_data_filter=nat_data[columns]\n",
    "    state_data_filter=state_data[columns]\n",
    "    \n",
    "    nat_data_filter=nat_data_filter.fillna(0)\n",
    "    state_data_filter=state_data_filter.fillna(0)\n",
    "    \n",
    "    index_to_remove=[]\n",
    "    for index, row in state_data_filter.iterrows():\n",
    "        #print(row['Province/State'])\n",
    "        if row['states'] in rows_to_drop:\n",
    "            index_to_remove.append(index)\n",
    "\n",
    "    state_data_filter=state_data_filter.drop(index=index_to_remove).reset_index()\n",
    "    #print(state_data_filter['states'].drop_duplicates())\n",
    "    #cols_for_hosp=['positiveIncrease','negativeIncrease','totalTestResultsIncrease','onVentilatorCurrently',\n",
    "     #              'inIcuCurrently','deathIncrease','recovered','hospitalizedIncrease','h_next1','h_next2',\n",
    "      #             'd_next1','d_next2']\n",
    "    cols_for_hosp=['positiveIncrease','negativeIncrease','totalTestResultsIncrease','onVentilatorCurrently',\n",
    "                   'inIcuCurrently','deathIncrease','recovered','hospitalizedIncrease']\n",
    "    \n",
    "    week_cases=hosp_cases_nat_state(nat_data_filter,state_data_filter,epiweek_date,week_save,state_index,cols_for_hosp,state_error,last_date)\n",
    "    \n",
    "    unit_test(week_cases,cols_for_hosp,epiweek_date,state_index,\"unit_test/hospitalization.csv\")\n",
    "    return week_cases,cols_for_hosp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_excess_death(inputdir,epiweek_date,state_index,dic_names_to_abbv,start_week,end_week,this_month,last_date):\n",
    "    data=pd.read_csv(inputdir+\"Excess_Deaths_COVID-19.csv\")\n",
    "#     cols_death=['Observed Number','Excess Higher Estimate']\n",
    "    cols_death = ['Observed Number','Excess Estimate']\n",
    "#     out_cols = ['Observed Numberv2','Excess Estimatev2']\n",
    "    cols=['Week Ending Date','State']+cols_death\n",
    "    data=data[cols]\n",
    "    data[cols]=data[cols].fillna(0)\n",
    "    state_names=list(dic_names_to_abbv.keys())\n",
    "    week_cases={}\n",
    "    for c in cols_death:\n",
    "        week_cases[c]=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    \n",
    "    for ix, row in data.iterrows():\n",
    "        week=row['Week Ending Date']\n",
    "        y,m,d=week.split('-')\n",
    "        cm,cd,cy=int(m),int(d),int(y)\n",
    "        name=row['State']\n",
    "        if name in state_names and cy>=2020: #and cm<=this_month:\n",
    "            #print(name)\n",
    "            #print(group)\n",
    "            cur_date=y+'-'+m+'-'+d\n",
    "            w_idx=find_same_week(epiweek_date,cur_date,last_date,date_string=False)\n",
    "            if w_idx!=-1:\n",
    "                state_id=state_index[dic_names_to_abbv[name]]\n",
    "                for c in cols_death:\n",
    "                    #if name=='Alabama':\n",
    "                     #   print(name,cur_date,w_idx,row[c])\n",
    "                    week_cases[c][state_id][w_idx]+=int(row[c])\n",
    "                    week_cases[c][0][w_idx]+=int(row[c])\n",
    "    \n",
    "    \n",
    "    ##ADD NAN VALUES TO THE LAST 2 WEEK\n",
    "    for s in range(len(state_names)):\n",
    "        for c in cols_death:\n",
    "            week_cases[c][s]/=3\n",
    "            week_cases[c][s][-1]=np.nan\n",
    "            week_cases[c][s][-2]=np.nan\n",
    "    \n",
    "    \n",
    "    unit_test(week_cases,cols_death,epiweek_date,state_index,\"unit_test/excess-death-weekly.csv\")\n",
    "    return week_cases,cols_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jhu_cases(inputdir,epiweek_date,state_index,dic_names_to_abbv):\n",
    "    data=pd.read_csv(inputdir+\"time_series_covid19_confirmed_US.csv\")\n",
    "    state_names=list(dic_names_to_abbv.keys())\n",
    "    dates_list=list(data.columns)\n",
    "    dates_list=dates_list[11:]\n",
    "    #print(state_names)\n",
    "    #print(dates_list)\n",
    "    #not_added_rows=['Diamond Princess','Grand Princess','Northern Mariana Islands','Guam','Hawaii',\n",
    "                   # 'Puerto Rico','Virgin Islands','American Samoa']\n",
    "    not_added_rows=[]\n",
    "    week_cases={}\n",
    "    cols=['positiveIncr_cumulative','positiveIncr']\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    \n",
    "    for ix, row in data.iterrows():\n",
    "        name=row['Province_State']\n",
    "        if name in state_names:\n",
    "            state_id=state_index[dic_names_to_abbv[name]]\n",
    "            for date in dates_list:\n",
    "                w_idx=get_current_week(epiweek_date,date,dates_list[-1])\n",
    "                #if date==dates_list[-1]:\n",
    "                 #   print(w_idx)\n",
    "                if w_idx!=-1:\n",
    "                    #print(name,date,row[date])\n",
    "                    week_cases[cols[0]][state_id][w_idx]+=int(row[date])\n",
    "                    week_cases[cols[0]][0][w_idx]+=int(row[date])\n",
    "        elif name not in not_added_rows: #if not state_names still adding them for national\n",
    "            #print('state name not added:'+name)\n",
    "            for date in dates_list:\n",
    "                w_idx=get_current_week(epiweek_date,date,dates_list[-1])\n",
    "                if w_idx!=-1:\n",
    "                    week_cases[cols[0]][0][w_idx]+=int(row[date])\n",
    "    \n",
    "    \n",
    "    #count incidence for the national+states\n",
    "    for state_id in range(len(state_names)):\n",
    "        week_cases[cols[1]][state_id][0]=int(week_cases[cols[0]][state_id][0])\n",
    "        for w_idx in range(1,len(epiweek_date)):\n",
    "            week_cases[cols[1]][state_id][w_idx]=int(week_cases[cols[0]][state_id][w_idx])-int(week_cases[cols[0]][state_id][w_idx-1])\n",
    "             \n",
    "    \n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/jhu-cases.csv\")\n",
    "    return week_cases,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jhu_death(inputdir,epiweek_date,state_index,dic_names_to_abbv,start_week=None,end_week=None):\n",
    "    data=pd.read_csv(inputdir+\"time_series_covid19_deaths_US.csv\")\n",
    "    state_names=list(dic_names_to_abbv.keys())\n",
    "    dates_list=list(data.columns)\n",
    "    dates_list=dates_list[12:]\n",
    "    #print(state_names)\n",
    "    #print(dates_list)\n",
    "    #not_added_rows=['Diamond Princess','Grand Princess','Northern Mariana Islands','Guam','Hawaii',\n",
    "                   # 'Puerto Rico','Virgin Islands','American Samoa']\n",
    "    not_added_rows=[]\n",
    "    week_cases={}\n",
    "    cols=['death_jhu_cumulative','death_jhu_incidence']\n",
    "    for c in cols:\n",
    "        week_cases[c]=np.zeros((len(state_names),len(epiweek_date)))\n",
    "    \n",
    "    for ix, row in data.iterrows():\n",
    "        name=row['Province_State']\n",
    "        if name in state_names:\n",
    "            state_id=state_index[dic_names_to_abbv[name]]\n",
    "            for date in dates_list:\n",
    "                w_idx=get_current_week(epiweek_date,date,dates_list[-1])\n",
    "                #if date==dates_list[-1]:\n",
    "                 #   print(w_idx)\n",
    "                if w_idx!=-1:\n",
    "                    #print(name,date,row[date])\n",
    "                    week_cases[cols[0]][state_id][w_idx]+=int(row[date])\n",
    "                    week_cases[cols[0]][0][w_idx]+=int(row[date])\n",
    "        elif name not in not_added_rows: #if not state_names still adding them for national\n",
    "            #print('state name not added:'+name)\n",
    "            for date in dates_list:\n",
    "                w_idx=get_current_week(epiweek_date,date,dates_list[-1])\n",
    "                if w_idx!=-1:\n",
    "                    week_cases[cols[0]][0][w_idx]+=int(row[date])\n",
    "    \n",
    "    \n",
    "    #count incidence for the national+states\n",
    "    for state_id in range(len(state_names)):\n",
    "        week_cases[cols[1]][state_id][0]=int(week_cases[cols[0]][state_id][0])\n",
    "        for w_idx in range(1,len(epiweek_date)):\n",
    "            week_cases[cols[1]][state_id][w_idx]=int(week_cases[cols[0]][state_id][w_idx])-int(week_cases[cols[0]][state_id][w_idx-1])\n",
    "             \n",
    "    \n",
    "    unit_test(week_cases,cols,epiweek_date,state_index,\"unit_test/jhu-death.csv\")\n",
    "    return week_cases,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_level_cases(inputdir,week_state,state_index,epiweek_date,cols, is_covidnet=False):\n",
    "    otherdatasir=inputdir+\"other_data/\"\n",
    "    week_hhs={}\n",
    "    num_region=11\n",
    "    state_names=list(state_index.keys())\n",
    "    for c in cols:\n",
    "        week_hhs[c]=np.zeros((num_region,len(epiweek_date)))\n",
    "    \n",
    "    file = open(otherdatadir+\"hhs_regions_abbv.txt\", 'r') \n",
    "    Lines = file.readlines() \n",
    "    state_hhs_map={}\n",
    "    state_pop=np.zeros(len(state_names))\n",
    "    population=open(otherdatadir+\"state_population.csv\",'r')\n",
    "    total=0  \n",
    "    popu_region=np.zeros(num_region)   \n",
    "    \n",
    "    ######## Getting population. Need this for weighted count ###########################        \n",
    "    \n",
    "    Lines_pop=population.readlines()\n",
    "    for line in Lines_pop:\n",
    "        state,popu,abbv=line.strip().split(',')\n",
    "        if state=='ttl':\n",
    "            state_pop[0]=float(popu)\n",
    "        else:\n",
    "            if state_index.get(abbv,-1)!=-1:\n",
    "                state_id=state_index[abbv]\n",
    "                state_pop[state_id]=float(popu)\n",
    "            else:\n",
    "                print('population state not found '+state)\n",
    "    \n",
    "    #### mapping each state to a hhs region ###\n",
    "    state_hhs_map[0]=0 # setting national value\n",
    "    popu_region[0]=1\n",
    "    for line in Lines:\n",
    "        regions=line.strip().split(',')\n",
    "        reg_id=int(regions[0])\n",
    "        for j in range(1,len(regions)):\n",
    "            if state_index.get(regions[j],-1)!=-1:\n",
    "                state_id=state_index[regions[j]]\n",
    "                state_hhs_map[state_id]=reg_id\n",
    "                popu_region[reg_id]+=state_pop[state_id]\n",
    "            else:\n",
    "                print('state not found '+regions[j])\n",
    "    \n",
    "    for key in state_hhs_map.keys():\n",
    "        hhs_id=int(state_hhs_map[int(key)])\n",
    "        for j in range(len(epiweek_date)):\n",
    "            for c in cols:\n",
    "                if is_covidnet and int(key)>0:\n",
    "                    if math.isnan(week_state[c][int(key)][j])==False:\n",
    "                        #print(int(key),week_state[c][int(key)][j])\n",
    "                        week_hhs[c][hhs_id][j]+=(week_state[c][int(key)][j]*state_pop[int(key)])/100000\n",
    "                else:    \n",
    "                    week_hhs[c][hhs_id][j]+=week_state[c][int(key)][j]\n",
    "    if is_covidnet:\n",
    "        for r in range(1,num_region):\n",
    "            for c in cols:\n",
    "                week_hhs[c][r][:]/=popu_region[r]\n",
    "            \n",
    "    return week_hhs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_region(mobility,kinsadir,iqvia,covidnet,hosp,cols_m,cols_k,cols_q,cols_net,cols_hosp,epiweek,\n",
    "               epiweek_date,outputdir,otherdatadir,outfilename,state_index,kinsa_start,kinsa_end):\n",
    "    r='Region '\n",
    "    region_names=['X']\n",
    "    for i in range(1,11):\n",
    "        region_names.append(r+str(i))\n",
    "    region_index={region_names[i]:i for i in range(len(region_names))}\n",
    "    cols_common=['date','epiweek','region']\n",
    "    all_cols=cols_common+cols_m+cols_k+cols_q+cols_net+cols_hosp\n",
    "    \n",
    "    mobility_hhs=region_level_cases(otherdatadir,mobility,state_index,epiweek_date,cols_m)\n",
    "    kinsa_hhs,cols_kinsa=read_kinsa(kinsadir,epiweek_date,region_index,kinsa_start,kinsa_end,isregion=True)\n",
    "    iqvia_hhs=region_level_cases(otherdatadir,iqvia,state_index,epiweek_date,cols_q)\n",
    "    covidnet_hhs=region_level_cases(otherdatadir,covidnet,state_index,epiweek_date,cols_net,is_covidnet=True)\n",
    "    hosp_hhs=region_level_cases(otherdatadir,hosp,state_index,epiweek_date,cols_hosp)\n",
    "    \n",
    "    #print(covidnet_hhs[cols_net[0]][1:])\n",
    "    \n",
    "    final_data=pd.DataFrame(columns=all_cols)\n",
    "    for reg in range(len(region_names)):\n",
    "        temp_data=pd.DataFrame(columns=all_cols)\n",
    "        temp_data['date']=epiweek_date\n",
    "        temp_data['epiweek']=epiweek\n",
    "        temp_data['region']=[region_names[reg]]*len(epiweek_date)\n",
    "        for c in cols_m:\n",
    "            temp_data[c]=mobility_hhs[c][reg][:]\n",
    "        for c in cols_kinsa:\n",
    "            temp_data[c]=kinsa_hhs[c][reg][:]\n",
    "        for c in cols_q:\n",
    "            temp_data[c]=iqvia_hhs[c][reg][:]\n",
    "        for c in cols_net:\n",
    "            temp_data[c]=covidnet_hhs[c][reg][:]\n",
    "        for c in cols_hosp:\n",
    "            temp_data[c]=hosp_hhs[c][reg][:]\n",
    "        \n",
    "        temp_data=temp_data[all_cols]\n",
    "        final_data=final_data.append(temp_data,ignore_index=True)\n",
    "    final_data=final_data[all_cols]\n",
    "    print(final_data.shape)\n",
    "    final_data.to_csv(outputdir+outfilename,index=False)\n",
    "    \n",
    "    print('FINISHED....')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_state(mobility,apple,vacc,vac_delphi,cdc_hosp,flu_hosp,dex,kinsa,covidnet,hosp,excess,jhu,survey,em_visit,jhu_case,hosp_new_res,\n",
    "                     cols_m,cols_a,cols_vacc,cols_vac_delphi,cols_cdc,cols_flu,cols_d,cols_k,cols_net,\n",
    "                     cols_hosp,cols_excess,cols_jhu,cols_survey,cols_v,cols_jhu_case,cols_hosp_new_res,\n",
    "                     state_fips,epiweek,epiweek_date,region_names,outputdir,outfilename):\n",
    "    \n",
    "    cols_common=['date','epiweek','region','fips']\n",
    "    #all_cols=cols_common+cols_m+cols_a+cols_d+cols_k+cols_net+cols_hosp+cols_excess+cols_jhu+cols_survey+cols_v\n",
    "    #all_cols=cols_common+cols_m+cols_a+cols_cdc+cols_d+cols_k+cols_q+cols_net+cols_hosp+cols_excess+cols_jhu+cols_survey+cols_v\n",
    "    all_cols=cols_common+cols_m+cols_a+cols_cdc+cols_flu+cols_d+cols_vacc+cols_vac_delphi+cols_k+cols_net+cols_hosp+cols_excess+cols_jhu+cols_survey+cols_v+cols_jhu_case+cols_hosp_new_res\n",
    "    \n",
    "    print(all_cols)\n",
    "    final_data=pd.DataFrame(columns=all_cols)\n",
    "    for reg in range(len(region_names)):\n",
    "        temp_data=pd.DataFrame(columns=all_cols)\n",
    "        temp_data['date']=epiweek_date\n",
    "        temp_data['epiweek']=epiweek\n",
    "        temp_data['region']=[region_names[reg]]*len(epiweek_date)\n",
    "        temp_data['fips']=[state_fips[region_names[reg]]]*len(epiweek_date)\n",
    "        for c in cols_m:\n",
    "            temp_data[c]=mobility[c][reg][:]\n",
    "        for c in cols_a:\n",
    "            temp_data[c]=apple[c][reg][:]\n",
    "        for c in cols_cdc:\n",
    "            temp_data[c]=cdc_hosp[c][reg][:]\n",
    "        for c in cols_flu:\n",
    "            temp_data[c]=flu_hosp[c][reg][:]\n",
    "        for c in cols_d:\n",
    "            temp_data[c]=dex[c][reg][:]\n",
    "        for c in cols_vacc:\n",
    "            temp_data[c]=vacc[c][reg][:]\n",
    "        for c in cols_vacc_delphi:\n",
    "            temp_data[c]=vacc_delphi[c][reg][:]\n",
    "        for c in cols_k:\n",
    "            temp_data[c]=kinsa[c][reg][:]\n",
    "        #for c in cols_q:\n",
    "         #   temp_data[c]=iqvia[c][reg][:]\n",
    "        for c in cols_net:\n",
    "            temp_data[c]=covidnet[c][reg][:]\n",
    "        for c in cols_hosp:\n",
    "            temp_data[c]=hosp[c][reg][:]\n",
    "        for c in cols_excess:\n",
    "            temp_data[c]=excess[c][reg][:]\n",
    "        for c in cols_jhu:\n",
    "            temp_data[c]=jhu[c][reg][:]\n",
    "        for c in cols_survey:\n",
    "            temp_data[c]=survey[c][reg][:]\n",
    "        for c in cols_v:\n",
    "            temp_data[c]=em_visit[c][reg][:]\n",
    "        for c in cols_jhu_case:\n",
    "            temp_data[c]=jhu_case[c][reg][:]\n",
    "        for c in cols_hosp_new_res:\n",
    "            temp_data[c]=hosp_new_res[c][reg][:]\n",
    "        \n",
    "        temp_data=temp_data[all_cols]\n",
    "        final_data=final_data.append(temp_data,ignore_index=True)\n",
    "    \n",
    "    final_data=final_data[all_cols]\n",
    "    print(final_data.shape)\n",
    "    final_data.to_csv(outputdir+outfilename,index=False)\n",
    "    \n",
    "    print('FINISHED....')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202204\n",
      "['2020-01-04', '2020-01-11', '2020-01-18', '2020-01-25', '2020-02-01', '2020-02-08', '2020-02-15', '2020-02-22', '2020-02-29', '2020-03-07', '2020-03-14', '2020-03-21', '2020-03-28', '2020-04-04', '2020-04-11', '2020-04-18', '2020-04-25', '2020-05-02', '2020-05-09', '2020-05-16', '2020-05-23', '2020-05-30', '2020-06-06', '2020-06-13', '2020-06-20', '2020-06-27', '2020-07-04', '2020-07-11', '2020-07-18', '2020-07-25', '2020-08-01', '2020-08-08', '2020-08-15', '2020-08-22', '2020-08-29', '2020-09-05', '2020-09-12', '2020-09-19', '2020-09-26', '2020-10-03', '2020-10-10', '2020-10-17', '2020-10-24', '2020-10-31', '2020-11-07', '2020-11-14', '2020-11-21', '2020-11-28', '2020-12-05', '2020-12-12', '2020-12-19', '2020-12-26', '2021-01-02', '2021-01-09', '2021-01-16', '2021-01-23', '2021-01-30', '2021-02-06', '2021-02-13', '2021-02-20', '2021-02-27', '2021-03-06', '2021-03-13', '2021-03-20', '2021-03-27', '2021-04-03', '2021-04-10', '2021-04-17', '2021-04-24', '2021-05-01', '2021-05-08', '2021-05-15', '2021-05-22', '2021-05-29', '2021-06-05', '2021-06-12', '2021-06-19', '2021-06-26', '2021-07-03', '2021-07-10', '2021-07-17', '2021-07-24', '2021-07-31', '2021-08-07', '2021-08-14', '2021-08-21', '2021-08-28', '2021-09-04', '2021-09-11', '2021-09-18', '2021-09-25', '2021-10-02', '2021-10-09', '2021-10-16', '2021-10-23', '2021-10-30', '2021-11-06', '2021-11-13', '2021-11-20', '2021-11-27', '2021-12-04', '2021-12-11', '2021-12-18', '2021-12-25', '2022-01-01', '2022-01-08', '2022-01-15', '2022-01-22', '2022-01-29']\n",
      "['202001', '202002', '202003', '202004', '202005', '202006', '202007', '202008', '202009', '202010', '202011', '202012', '202013', '202014', '202015', '202016', '202017', '202018', '202019', '202020', '202021', '202022', '202023', '202024', '202025', '202026', '202027', '202028', '202029', '202030', '202031', '202032', '202033', '202034', '202035', '202036', '202037', '202038', '202039', '202040', '202041', '202042', '202043', '202044', '202045', '202046', '202047', '202048', '202049', '202050', '202051', '202052', '202053', '202101', '202102', '202103', '202104', '202105', '202106', '202107', '202108', '202109', '202110', '202111', '202112', '202113', '202114', '202115', '202116', '202117', '202118', '202119', '202120', '202121', '202122', '202123', '202124', '202125', '202126', '202127', '202128', '202129', '202130', '202131', '202132', '202133', '202134', '202135', '202136', '202137', '202138', '202139', '202140', '202141', '202142', '202143', '202144', '202145', '202146', '202147', '202148', '202149', '202150', '202151', '202152', '202201', '202202', '202203', '202204']\n",
      "google mobility\n",
      "['retail_and_recreation_percent_change_from_baseline', 'grocery_and_pharmacy_percent_change_from_baseline', 'parks_percent_change_from_baseline', 'transit_stations_percent_change_from_baseline', 'workplaces_percent_change_from_baseline', 'residential_percent_change_from_baseline']\n",
      "(5559, 8)\n",
      "output file written\n",
      "51\n",
      "JHU-cases\n",
      "(5559, 4)\n",
      "output file written\n",
      "hosp-neg-total result\n",
      "(5559, 4)\n",
      "output file written\n",
      "Vaccine dose\n",
      "(5559, 4)\n",
      "output file written\n",
      "delphi vaccine survey\n",
      "smoothed_wcovid_vaccinated 1 success 3162\n",
      "smoothed_wtested_positive_14d 1 success 2752\n",
      "smoothed_wwearing_mask 1 success 2509\n",
      "smoothed_wtravel_outside_state_5d 1 success 515\n",
      "smoothed_wspent_time_1d 1 success 526\n",
      "(5559, 7)\n",
      "output file written\n",
      "FB-GOOGLE\n",
      "fb_wcli4 1 success 3017\n",
      "fb_wcli5 1 success 3021\n",
      "fb_wcli6 1 success 2949\n",
      "fb_wcli8 1 success 2687\n",
      "fb_wcli9 1 success 2830\n",
      "google_cli 1 success 1734\n",
      "fb_wili4 1 success 3017\n",
      "fb_wili5 1 success 3021\n",
      "fb_wili6 1 success 2949\n",
      "fb_wili8 1 success 2687\n",
      "fb_wili9 1 success 2830\n",
      "fb_wcli len 31463\n",
      "fb_wli len 31463\n",
      "(5559, 5)\n",
      "output file written\n",
      "CDC hospitalization\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "week index not found:2022-01-30\n",
      "30 1 2022\n",
      "(5559, 3)\n",
      "output file written\n",
      "apple mobility\n",
      "(5559, 3)\n",
      "output file written\n",
      "covidnet\n",
      "Alabama\n",
      "Alaska\n",
      "Arizona\n",
      "Arkansas\n",
      "California\n",
      "Colorado\n",
      "Connecticut\n",
      "Delaware\n",
      "District of Columbia\n",
      "Florida\n",
      "Georgia\n",
      "Idaho\n",
      "Illinois\n",
      "Indiana\n",
      "Iowa\n",
      "Kansas\n",
      "Kentucky\n",
      "Louisiana\n",
      "Maine\n",
      "Maryland\n",
      "Massachusetts\n",
      "Michigan\n",
      "Minnesota\n",
      "Mississippi\n",
      "Missouri\n",
      "Montana\n",
      "Nebraska\n",
      "Nevada\n",
      "New Hampshire\n",
      "New Jersey\n",
      "New Mexico\n",
      "New York\n",
      "North Carolina\n",
      "North Dakota\n",
      "Ohio\n",
      "Oklahoma\n",
      "Oregon\n",
      "Pennsylvania\n",
      "Rhode Island\n",
      "South Carolina\n",
      "South Dakota\n",
      "Tennessee\n",
      "Texas\n",
      "Utah\n",
      "Vermont\n",
      "Virginia\n",
      "Washington\n",
      "West Virginia\n",
      "Wisconsin\n",
      "Wyoming\n",
      "Entire Network\n",
      "(5559, 3)\n",
      "output file written\n",
      "excess death\n",
      "(5559, 4)\n",
      "output file written\n",
      "JHU\n",
      "(5559, 4)\n",
      "output file written\n",
      "covid exposure index:dex\n",
      "(5559, 28)\n",
      "output file written\n",
      "kinsa\n",
      "(5559, 3)\n",
      "output file written\n",
      "hospitalization\n",
      "(5559, 10)\n",
      "output file written\n",
      "emergency-visits\n",
      "Index(['region', 'Week', 'Number of Facilities Reporting',\n",
      "       'Total Number of ED Visits', 'CLI Percent of Total Visits',\n",
      "       'ILI Percent of Total Visits', 'Number of Sites Reporting',\n",
      "       'Total Patient Visits', '% of Total Visits for ILI (unweighted)',\n",
      "       '% of Total Visits for ILI (weighted)', ' Baseline', 'Unnamed: 11',\n",
      "       'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15'],\n",
      "      dtype='object')\n",
      "state not found PR\n",
      "state not found VI\n",
      "state not found HI\n",
      "state not found AS\n",
      "state not found MP\n",
      "state not found FM\n",
      "state not found GU\n",
      "state not found MH\n",
      "state not found RP\n",
      "{'X': 0, 'AL': 1, 'AK': 2, 'AZ': 3, 'AR': 4, 'CA': 5, 'CO': 6, 'CT': 7, 'DE': 8, 'DC': 9, 'FL': 10, 'GA': 11, 'ID': 12, 'IL': 13, 'IN': 14, 'IA': 15, 'KS': 16, 'KY': 17, 'LA': 18, 'ME': 19, 'MD': 20, 'MA': 21, 'MI': 22, 'MN': 23, 'MS': 24, 'MO': 25, 'MT': 26, 'NE': 27, 'NV': 28, 'NH': 29, 'NJ': 30, 'NM': 31, 'NY': 32, 'NC': 33, 'ND': 34, 'OH': 35, 'OK': 36, 'OR': 37, 'PA': 38, 'RI': 39, 'SC': 40, 'SD': 41, 'TN': 42, 'TX': 43, 'UT': 44, 'VT': 45, 'VA': 46, 'WA': 47, 'WV': 48, 'WI': 49, 'WY': 50}\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "(5559, 4)\n",
      "output file written\n",
      "merging all state..\n",
      "['date', 'epiweek', 'region', 'fips', 'retail_and_recreation_percent_change_from_baseline', 'grocery_and_pharmacy_percent_change_from_baseline', 'parks_percent_change_from_baseline', 'transit_stations_percent_change_from_baseline', 'workplaces_percent_change_from_baseline', 'residential_percent_change_from_baseline', 'apple_mobility', 'cdc_hospitalized', 'cdc_flu_hosp', 'dex', 'dex_a', 'dex_income_1', 'dex_income_1_a', 'dex_income_2', 'dex_income_2_a', 'dex_income_3', 'dex_income_3_a', 'dex_income_4', 'dex_income_4_a', 'dex_education_1', 'dex_education_1_a', 'dex_education_2', 'dex_education_2_a', 'dex_education_3', 'dex_education_3_a', 'dex_education_4', 'dex_education_4_a', 'dex_race_asian', 'dex_race_asian_a', 'dex_race_black', 'dex_race_black_a', 'dex_race_hispanic', 'dex_race_hispanic_a', 'dex_race_white', 'dex_race_white_a', 'Stage_One_Doses', 'Stage_Two_Doses', 'smoothed_wcovid_vaccinated', 'smoothed_wtested_positive_14d', 'smoothed_wwearing_mask', 'smoothed_wtravel_outside_state_5d', 'smoothed_wspent_time_1d', 'kinsa_cases', 'covidnet', 'positiveIncrease', 'negativeIncrease', 'totalTestResultsIncrease', 'onVentilatorCurrently', 'inIcuCurrently', 'deathIncrease', 'recovered', 'hospitalizedIncrease', 'Observed Number', 'Excess Estimate', 'death_jhu_cumulative', 'death_jhu_incidence', 'fb_survey_wcli', 'google_survey_cli', 'fb_survey_wili', 'Number of Facilities Reporting', 'CLI Percent of Total Visits', 'positiveIncr_cumulative', 'positiveIncr', 'negativeIncr', 'total_resultsIncr']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5559, 69)\n",
      "FINISHED....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\\'merging all region..\\')\\noutputdir=\"/Users/anikat/Downloads/covid-hospitalization-data/\"\\noutfile=\"covid-hospitalization-all-region-merged.csv\"\\nmerge_data_region(mobility_state,kinsa_path,iqvia_state,data_covidnet,data_hosp,cols_m,cols_k,cols_q,cols_net,cols_hosp,epiweek,\\n               week_save,outputdir,other_path,outfile,state_index,start_week_k,end_week_k)\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "### WHAT DATA ##\n",
    "1.To get the hospitalization data we need the following datasets:\n",
    "  a. mobility (filename:Global_Mobility_Report.csv)\n",
    "  b.kinsa (filename:kinsa_observedili_state_avg.csv)\n",
    "  c.iqvia (filename:iqvia_Processed.csv)\n",
    "  d.covidnet (filename:COVID-NET_Processed.csv)\n",
    "  e.covid-tracking (national, state)\n",
    "      datasets are collected from site: https://covidtracking.com/api\n",
    "        i. We collected US historical data (filename save: us-daily-hospitalizations.csv)\n",
    "        ii. State historical data (filename save: states-daily-hospitalizations.csv)\n",
    "  f. apple (applemobilitytrends.csv)\n",
    "  g. dex (state-dex.csv)\n",
    "  h. emergency visit (emeregency-vists.csv)\n",
    "  i. jhu deaths: time_series_covid19_deaths_US.csv\n",
    "  j. excess deaths: Excess_Deaths_COVID_19.csv\n",
    "  \n",
    "## WHERE DATA SHOULD BE KEPT AND WHAT NAME ###\n",
    "2. change all data directory as where it is kept\n",
    "3. DONT CHANGE FILENAME (keep in the same name as written above)\n",
    "4. There are some additional data like population, state-fips code etc required to process this file, \n",
    "  keep all of them in same directory as data_path/other_data\n",
    "\n",
    "### INPUT PARAMETERS TO PASS FOR PROCESSING EACH FILE ##\n",
    "5. At the beginning of every data to be processes also pass a input parameter as start_week/end_week or \n",
    "   both based on dataset. This is for consistency as not all dataset contain all epiweeks value \n",
    "\n",
    "6. change get_epiweek_list(start_week,end_week) also. These are the weeks to show in the final output file\n",
    "\n",
    "7. for hospitalization only change last_date instead of start/end_week. \n",
    "   last_date means the date which upto which data is available\n",
    "\n",
    "##OUTPUT FILE NAME AND DIRECTORY##\n",
    "8. merged_data(..) is merging all the columns. Change the filename and output directory in the fields \n",
    "   before calling the method\n",
    "\n",
    "'''\n",
    "import copy\n",
    "if date.today().weekday() != 6:\n",
    "    week_num = (date.today()-timedelta(2)).strftime(\"%U\")\n",
    "    year_week_num = \"2022\"  + week_num\n",
    "    print(year_week_num)\n",
    "    week_end_date = (date.today() +timedelta((5-date.today().weekday()) % 7 )).strftime('%Y-%m-%d')\n",
    "    week_end_date = (date.today() - timedelta(2)).strftime('%Y-%m-%d')\n",
    "    week_end_string = (date.today() + timedelta((5-date.today().weekday()) % 7 )).strftime('%Y%m%d')\n",
    "    week_end_string = (date.today() - timedelta(2)).strftime('%Y%m%d')\n",
    "\n",
    "else:\n",
    "    week_num = (date.today()-timedelta(days=1)).strftime(\"%U\")\n",
    "    year_week_num = \"2022\" + week_num\n",
    "    print(year_week_num)\n",
    "    week_end_date = (date.today() -timedelta((date.today().weekday()-5) % 7 )).strftime('%Y-%m-%d')\n",
    "    week_end_string = (date.today() - timedelta((date.today().weekday()-5) % 7 )).strftime('%Y%m%d')\n",
    "### INPUT FILE DIRECTORY #####\n",
    "# data_path=\"/Users/anikat/Downloads/covid-hospitalization-data/\"\n",
    "data_path=\"./\"\n",
    "# kinsa_path= \"/Users/anikat/Downloads/kinsahealth/\"\n",
    "kinsa_path = \"./\"\n",
    "### INPUT: EPIWEEK START, END WEEK ####\n",
    "#Both the method args will be same. This is to keep last date in the ouput file to same as epiweek\n",
    "epiweek1,epiweek_date1=get_epiweek_list('202001','202053',2020)\n",
    "epiweek2,epiweek_date2=get_epiweek_list('202101','202152',2021)\n",
    "epiweek3,epiweek_date3=get_epiweek_list('202201',year_week_num,2022)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epiweek=epiweek1+epiweek2+epiweek3\n",
    "epiweek_date=epiweek_date1+epiweek_date2+epiweek_date3\n",
    "week_save=copy.deepcopy(epiweek_date)\n",
    "### ****NOTE: for new year 2021 add another epiweek list and append with previous**** #######\n",
    "\n",
    "print(epiweek_date)\n",
    "print(epiweek)\n",
    "# PROCESSING MOBILITY ####\n",
    "# INPUT: END_WEEK: the last epiweek this csv file contains  ####\n",
    "print('google mobility')\n",
    "start_week_m=1\n",
    "end_week_m=int(week_num) #2021, m=epiweek since data generally have value -4 days lag for current week\n",
    "mobility_state,cols_m,state_index,dic_names_to_abbv=read_mobility(data_path,epiweek_date,end_week_m)\n",
    "dic_names_to_abbv['United States']='X'\n",
    "#print(dic_names_to_abbv)\n",
    "print(len(state_index.keys()))\n",
    "#print(state_index)\n",
    "\n",
    "print('JHU-cases')\n",
    "jhu_cases,cols_jhu_case=read_jhu_cases(data_path,epiweek_date,state_index,dic_names_to_abbv)\n",
    "\n",
    "print('hosp-neg-total result')\n",
    "hosp_new_res,cols_hosp_new_res=hosp_negative_total_data(data_path,epiweek_date,state_index)\n",
    "\n",
    "print('Vaccine dose')\n",
    "last_date=week_end_date\n",
    "vacc,cols_vacc=read_vaccine_doses(data_path,epiweek_date,state_index,dic_names_to_abbv,last_date)\n",
    "\n",
    "print('delphi vaccine survey')\n",
    "start_week=20210101\n",
    "end_week=int(week_end_string)  #yyyymmdd\n",
    "\n",
    "vacc_delphi,cols_vacc_delphi=read_delphi_vaccine(state_index,epiweek_date,start_week,end_week)\n",
    "\n",
    "### PROCESSING FB-GOOGLE ####\n",
    "### INPUT: start_week, end_week; start_week does not matter as both survey starts from 20200401, \n",
    "###  but always update end_week###\n",
    "print('FB-GOOGLE')\n",
    "start_week=20200307\n",
    "end_week=int(week_end_string) #yyyymmdd\n",
    "survey,cols_survey=read_delphi_fb_google_survey(state_index,epiweek_date,start_week,end_week)\n",
    "\n",
    "\n",
    "print('CDC hospitalization')\n",
    "end_week=int(week_num) #2021\n",
    "cdc_hosp,cols_cdc,flu_hosp,cols_flu=read_cdc_hosp(data_path,epiweek_date,state_index,end_week)\n",
    "\n",
    "### PROCESSING APPLE MOBILITY per date for Berkely guys####\n",
    "#read_apple_mobility_per_date(data_path,epiweek_date,state_index,dic_names_to_abbv,start_week_m,end_week_m)\n",
    "\n",
    "### PROCESSING APPLE MOBILITY ####\n",
    "### INPUT: START_WEEK:1,END_WEEK: the last epiweek this csv file contains  ####\n",
    "print('apple mobility')\n",
    "end_week_m=int(week_num)\n",
    "apple_mobility,cols_a=read_apple_mobility(data_path,epiweek_date,state_index,dic_names_to_abbv,start_week_m,end_week_m)\n",
    "\n",
    "\n",
    "### PROCESSING COVIDNET ####\n",
    "### INPUT: START, END_WEEK: have 1 weeks lag data, same as IQVIA; STEP: TILL which index covidnet starts###\n",
    "print('covidnet')\n",
    "step=9 #if epiweek starts from 10 then step=0, if epiweek starts from 1 step=9 (10-epiweek_start)\n",
    "start_week_n=10\n",
    "end_week_n=int(week_num) #0 is just for week 53, change to 1, since next week change it to epiweek-1 if data pulled after Sat\n",
    "data_covidnet,cols_net= read_covidnet_data(data_path,epiweek_date,state_index,dic_names_to_abbv,start_week_n,end_week_n,step)\n",
    "\n",
    "\n",
    "### PROCESSING excess death ####\n",
    "### INPUT: start_week, end_week, this_month: number of current month###\n",
    "print('excess death')\n",
    "start_week_e=1\n",
    "end_week_e=int(week_num) #epiweek\n",
    "this_month=1\n",
    "last_date=week_end_date #yyyy-mm-dd change to original epiweek date if pulled after Sat, else keep it as today's date\n",
    "excess_death,cols_excess=read_excess_death(data_path,epiweek_date,state_index,dic_names_to_abbv,start_week_e,end_week_e,this_month,last_date)\n",
    "\n",
    "### PROCESSING jhu death ####\n",
    "print('JHU')\n",
    "jhu_death,cols_jhu=read_jhu_death(data_path,epiweek_date,state_index,dic_names_to_abbv)\n",
    "\n",
    "\n",
    "##DO NOT CHANGE THESE 3, they stopped update\n",
    "\n",
    "### PROCESSING Covid exposure indices on income, race,etc. ####\n",
    "### INPUT: END_WEEK: the last epiweek this csv file contains  ####\n",
    "print('covid exposure index:dex')\n",
    "end_week_dex=16\n",
    "dex,cols_d=read_dex(data_path,epiweek_date,state_index,start_week_m,end_week=end_week_dex)\n",
    "\n",
    "### PROCESSING KINSA ####\n",
    "print('kinsa')\n",
    "start_week_k=1\n",
    "end_week_k=40 #if this data is 1 week lag, then epiweek-1, else epiweek\n",
    "kinsa_state,cols_k=read_kinsa(kinsa_path,epiweek_date,state_index,start_week_k,end_week_k)\n",
    "\n",
    "## PROCESSING HOSPITALIZATION ####\n",
    "print('hospitalization')\n",
    "last_date='20210307' #yyymmdd:change to original epiweek date if pulled after Sat, else keep it as yesterday date\n",
    "#states needs hosp current  \n",
    "state_error=['CA','DC','TX','IL','LA','PA','MI','MO','NC','NV','DE'] #'NJ', 'WA', 'NE'\n",
    "data_hosp,cols_hosp=read_hospitalization(data_path,data_path,epiweek_date,week_save,state_index,\n",
    "                                      last_date,state_error)\n",
    "\n",
    "# ## PROCESSING IQVIA ####\n",
    "# ## INPUT: START, END_WEEK: have 2 weeks lag data###\n",
    "# print('iqvia')\n",
    "# change start, end week based on data and epiweek_date. current data has value since epiweek 10-17\n",
    "# start_week_q=1\n",
    "# end_week_q=33 #if this data is 1 week lag, then epiweek-1, else epiweek\n",
    "# iqvia_state,cols_q=read_iqvia(data_path,epiweek_date,state_index,start_week_q,end_week_q)\n",
    "\n",
    "## PROCESSING Emergency-visits ####\n",
    "print('emergency-visits')\n",
    "start_week_v=202001\n",
    "end_week_v=202105\n",
    "em_visit,cols_v=read_emergency(data_path,epiweek_date,state_index,start_week_v,end_week_v)\n",
    "\n",
    "state_names=list(state_index.keys())\n",
    "state_fips=read_fips_code(state_names,data_path)\n",
    "\n",
    "\n",
    "# #'''\n",
    "# #cdc_hosp={}\n",
    "# #cols_cdc=\"\"\n",
    "print('merging all state..')\n",
    "#Change outputdir and outfilename with the path and name of out file\n",
    "outputdir=data_path #\"/Users/anikat/Downloads/covid-hospitalization-data/\"\n",
    "outfile=\"covid-hospitalization-all-state-merged_vEW\" + year_week_num+\".csv\"\n",
    "merge_data_state(mobility_state,apple_mobility,vacc,vacc_delphi,cdc_hosp,flu_hosp,dex,kinsa_state,data_covidnet,data_hosp,excess_death,\n",
    "                 jhu_death,survey,em_visit,jhu_cases,hosp_new_res,\n",
    "                 cols_m,cols_a,cols_vacc,cols_vacc_delphi,cols_cdc,cols_flu,cols_d,cols_k,cols_net,cols_hosp,cols_excess,\n",
    "                 cols_jhu,cols_survey,cols_v,cols_jhu_case,cols_hosp_new_res,\n",
    "                 state_fips,epiweek,week_save,state_names,outputdir,outfile)\n",
    "\n",
    "\n",
    "# #'''\n",
    "\n",
    "# kinsa_state,data_hosp, em_visit,cols_k, cols_hosp,cols_v,\n",
    "\n",
    "'''\n",
    "print('merging all region..')\n",
    "outputdir=\"/Users/anikat/Downloads/covid-hospitalization-data/\"\n",
    "outfile=\"covid-hospitalization-all-region-merged.csv\"\n",
    "merge_data_region(mobility_state,kinsa_path,iqvia_state,data_covidnet,data_hosp,cols_m,cols_k,cols_q,cols_net,cols_hosp,epiweek,\n",
    "               week_save,outputdir,other_path,outfile,state_index,start_week_k,end_week_k)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-01-08']\n",
      "['202201']\n"
     ]
    }
   ],
   "source": [
    "#epiweek,epiweek_date=get_epiweek_list('202001','202053',2020)\n",
    "epiweek,epiweek_date=get_epiweek_list('202101','202201',2022)\n",
    "print(epiweek_date)\n",
    "print(epiweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anikat/Downloads/covid-hospitalization-data/jhu_deaths/05-09-2020.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-5d8a8ac4239d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mget_jhu_deaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-67-5d8a8ac4239d>\u001b[0m in \u001b[0;36mget_jhu_deaths\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#for file in glob.glob(outputdir+\"*.csv\"):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"05-09-2020.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Province_State'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Country_Region'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Deaths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cx4242/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cx4242/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cx4242/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cx4242/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cx4242/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/anikat/Downloads/covid-hospitalization-data/jhu_deaths/05-09-2020.csv'"
     ]
    }
   ],
   "source": [
    "##OPTIONAL: NO NEED TO RUN THIS FOR MERGING\n",
    "def get_jhu_deaths():\n",
    "    outputdir=\"/Users/anikat/Downloads/covid-hospitalization-data/jhu_deaths/\"\n",
    "    row_to_remove=['American Samoa','Grand Princess', 'Diamond Princess', 'Northern Mariana Islands', \n",
    "                   'Virgin Islands','Puerto Rico','Guam','Hawaii']\n",
    "    fips_path=\"/Users/anikat/Downloads/covid-hospitalization-data/other_data/\"\n",
    "    #allfiles = []\n",
    "    week='2020-05-09'\n",
    "    death_st=np.zeros(51)\n",
    "    death_nat=0\n",
    "    flag=True\n",
    "    #for file in glob.glob(outputdir+\"*.csv\"):\n",
    "    data=pd.read_csv(outputdir+\"05-09-2020.csv\",delimiter=',')\n",
    "    data=data[['Province_State','Country_Region','Deaths']]\n",
    "        \n",
    "    #if flag:\n",
    "    state_names=list(data['Province_State'].drop_duplicates().values)\n",
    "    for r in row_to_remove:\n",
    "        state_names.remove(r)\n",
    "    state_names.append('X')\n",
    "    print(state_names)\n",
    "    print(len(state_names))\n",
    "    dic_names_to_abbv=read_fips_code(state_names,fips_path,abbv_to_code=False)\n",
    "    state_index={dic_names_to_abbv[state_names[i]]:i for i in range(len(state_names))}\n",
    "    print(state_index)\n",
    "    for ix, row in data.iterrows():\n",
    "        if row['Province_State'] in row_to_remove:\n",
    "            continue\n",
    "        st_id=state_index[dic_names_to_abbv[row['Province_State']]]\n",
    "        death_st[st_id]+=int(row['Deaths'])\n",
    "        death_nat+=int(row['Deaths'])\n",
    "        \n",
    "    print('-nat-death')\n",
    "    print(death_nat)\n",
    "    flag=False\n",
    "    print(death_st)\n",
    "    outdata=pd.DataFrame(columns=['date','state','deaths'])\n",
    "    outdata['date']=[week]\n",
    "    outdata['state']=['X']\n",
    "    outdata['deaths']=[death_nat]\n",
    "    for st in state_index.keys():\n",
    "        tmpdata=pd.DataFrame(columns=['date','state','deaths'])\n",
    "        tmpdata['date']=[week]\n",
    "        tmpdata['state']=[st]\n",
    "        tmpdata['deaths']=[int(death_st[state_index[st]])]\n",
    "        outdata=outdata.append(tmpdata,ignore_index=True)\n",
    "    \n",
    "    print(outdata)\n",
    "    outdata.to_csv(fips_path+\"jhu_deaths.csv\",index=False)\n",
    "       \n",
    "    \n",
    "get_jhu_deaths()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_res1 = Epidata.covidcast('fb-survey', 'raw_cli', 'day', 'state', [20200307, Epidata.range(20200307, 20200601)], '*')\n",
    "fb_res2 = Epidata.covidcast('fb-survey', 'raw_cli', 'day', 'state', [20200601, Epidata.range(20200601, 20200701)], '*')\n",
    "fb_res3 = Epidata.covidcast('fb-survey', 'raw_cli', 'day', 'state', [20200701, Epidata.range(20200701, 20200815)], '*')\n",
    "from datetime import date\n",
    "fb_res_wili = Epidata.covidcast('fb-survey', 'raw_wili', 'day', 'state', [20201230, Epidata.range(20210101, 20210102)], '*')\n",
    "print(fb_res1.keys())\n",
    "print(fb_res2.keys())\n",
    "print(fb_res3.keys())\n",
    "#print(fb_res1['result'],fb_res1['message']) \n",
    "#print(fb_res2['result'],fb_res2['message'])\n",
    "#print(fb_res3['result'],fb_res3['message'])\n",
    "#'''\n",
    "print(fb_res1['result'], fb_res1['message'], len(fb_res1['epidata']))\n",
    "print(fb_res2['result'], fb_res2['message'], len(fb_res2['epidata']))\n",
    "print(fb_res3['result'], fb_res3['message'], len(fb_res3['epidata']))\n",
    "print(fb_res_wili['result'], fb_res_wili['message'], len(fb_res_wili['epidata']))\n",
    "fb_res=fb_res1['epidata']+fb_res2['epidata']\n",
    "fb_cli=fb_res+fb_res3['epidata']\n",
    "#print(fb_res1['epidata'][0])\n",
    "#print(len(fb_cli))\n",
    "print(fb_res_wili)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epiweek1,epiweek_date1=get_epiweek_list('202001','202053',2020)\n",
    "epiweek2,epiweek_date2=get_epiweek_list('202101','202152',2021)\n",
    "epiweek3,epiweek_date3=get_epiweek_list('202201','202252',2022)\n",
    "\n",
    "\n",
    "epiweek=epiweek1+epiweek2+epiweek3\n",
    "epiweek_date=epiweek_date1+epiweek_date2+epiweek_date3\n",
    "\n",
    "epiweek_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('delphi vaccine survey')\n",
    "start_week=20210101\n",
    "end_week=int(year_week_num)  #yyyymmdd\n",
    "\n",
    "vacc_delphi,cols_vacc_delphi=read_delphi_vaccine(state_index,epiweek_date,start_week,end_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
